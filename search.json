[
  {
    "objectID": "paper_results.html",
    "href": "paper_results.html",
    "title": "Paper - Code Analysis Results",
    "section": "",
    "text": "In order to construct our dataset, we used the GPL 3.0 version of the codeparrot/github-code dataset as the models we are evaluating explicitly remove GPL licensed code, so there is a less chance of data leakage. Additionally, we filter out code that goes beyond 4096 characters long as the AST parser we use can be slow for very long code snippets. We also filter out code that is not written in Python, as we are only evaluating Python code models.\n\nfrom datasets import load_dataset\n\nds = load_dataset(\n    \"bigcode/the-stack-smol\",\n    split=\"train\",\n    data_dir=\"data/python\",\n    cache_dir=\"/work/.cache/huggingface/datasets\",\n)\n\nfiltered_ds = ds.filter(lambda example: len(example[\"content\"]) < 4096)\nlen(filtered_ds)\n\nDownloading and preparing dataset json/bigcode--the-stack-smol to /work/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n\n\n\n\n\n\n\n\n\n\n\nDataset json downloaded and prepared to /work/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n\n\n\n\n\n6080\n\n\nNext we filter out repositories that do not have multiple files in the dataset as we are primarly concerned with internal vs.Â external method call prediction performance. We then extract all the method definitions and invocations and filter out any examples that do not have internal method invocations. Will also filter out internal methods that are very common such as get and set. Lastly, we ensure there is an even representation of internal and external method calls in the dataset.\n\ndef find_duplicates(items):\n    # Create an empty set to store the items that we have already seen\n    seen = set()\n\n    # Create an empty list to store the duplicates that we find\n    duplicates = []\n\n    # Loop through each item in the list\n    for item in items:\n        # If the item is already in the \"seen\" set, then it must be a duplicate\n        if item in seen:\n            # Add the duplicate to the list\n            duplicates.append(item)\n        # If the item is not in the \"seen\" set, then add it to the set\n        else:\n            seen.add(item)\n\n    # Return the list of duplicates\n    return duplicates\n\n\nrepo_names = find_duplicates(filtered_ds[\"repository_name\"])\n\nrepo_files = {}\nfor repo_name in repo_names:\n    rows_w_repo = filtered_ds.filter(\n        lambda example: example[\"repository_name\"] == repo_name\n    )\n\n    if len(rows_w_repo) > 1:\n        repo_files[repo_name] = [row[\"content\"] for row in rows_w_repo]\n        if len(repo_files) > 400:\n            break\n\n# filter out repos with only one file\nfiltered_ds = filtered_ds.filter(\n    lambda example: example[\"repository_name\"] in repo_files\n)\nlen(filtered_ds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n374\n\n\n\nfrom code_tokenizers.core import CodeTokenizer\nfrom transformers import AutoModelForCausalLM\n\nmodel_name = \"bigcode/santacoder\"\npy_tokenizer = CodeTokenizer.from_pretrained(\n    model_name, \"python\", padding_token=\"<|endoftext|>\"\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\npy_tokenizer.tokenizer\n\nExplicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\nExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n\n\nPreTrainedTokenizerFast(name_or_path='bigcode/santacoder', vocab_size=49152, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'pad_token': '<|endoftext|>'})\n\n\n\nfrom code_tokenizers.helpers import get_internal_methods\n\n\ndef example_internal_methods(example):\n    internal_methods = get_internal_methods([example[\"content\"]], py_tokenizer)\n    repo_internal_methods = get_internal_methods(\n        repo_files[example[\"repository_name\"]], py_tokenizer\n    )\n    repo_internal_methods = repo_internal_methods - internal_methods\n    return repo_internal_methods\n\n\n# add the internal methods to the dataset\nfiltered_ds = filtered_ds.map(\n    lambda example: {\"internal_methods\": example_internal_methods(example)}\n)\n# filter out repos with no internal methods\nfiltered_ds = filtered_ds.filter(lambda example: len(example[\"internal_methods\"]) > 0)\nlen(filtered_ds)\n\n\n\n\n\n\n\n293\n\n\n\nfrom functools import partial\nfrom transformers import default_data_collator\n\n\ndef code_collator(batch):\n    merged_ast = []\n    for b in batch:\n        merged_ast.append(b.pop(\"merged_ast\"))\n\n    batch = default_data_collator(batch)\n    batch[\"merged_ast\"] = merged_ast\n    return batch\n\n\ndef tokenizer_wrapper(tokenizer, example, column, *args, **kwargs):\n    return tokenizer(\n        example[column], internal_methods=example[\"internal_methods\"], *args, **kwargs\n    )\n\n\n# Setup tokenizer\ntokenizer = partial(tokenizer_wrapper, py_tokenizer, column=\"content\")\ntokenizer.decode = py_tokenizer.decode\n\n\nfrom perplexed.core import perplexed\n\ncross_dist, token_cnt = perplexed(\n    model.cuda(),\n    filtered_ds,\n    tokenizer=tokenizer,\n    column=\"content\",\n    semantic_column=\"merged_ast\",\n    batch_size=4,\n    num_proc=32,\n    device=\"cuda\",\n    collate_fn=code_collator,\n    pass_row=True,\n    return_tokens=True,\n    return_distributions=True,\n    compute_perplexity=False,\n)\n\n                                                                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n\ndef visualize_perplexities(perplexities, tokens, title, filename):\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax = sns.boxplot(data=perplexities, palette=\"Set2\")\n    ax.set_xticklabels(tokens)\n    ax.set_title(title)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.show()\n\n\nmost_common = token_cnt.most_common()\nmethod_invocations = [\n    t\n    for t in most_common\n    if t[0].startswith(\"<argument_list\") or t[0].startswith(\"<call\")\n]\ninternals = [t for t in method_invocations if \"internal\" in t[0]]\nexternals = [t for t in method_invocations if \"internal\" not in t[0]]\n\n\n# sort by name\ninternals = sorted(internals, key=lambda x: x[0])\nexternals = sorted(externals, key=lambda x: x[0])\n\n\ninternal_crosses = [cross_dist[token] for token, _ in internals]\nexternal_crosses = [cross_dist[token] for token, _ in externals]\n\n\nvisualize_perplexities(\n    internal_crosses, internals, \"Internal Method Calls\", \"internal_method_calls.png\"\n)\n\n\n\n\n\nvisualize_perplexities(\n    external_crosses, externals, \"External Method Calls\", \"external_method_calls.png\"\n)\n\n\n\n\n\nast_tokens = [\n    t\n    for t in most_common\n    if (t[0].startswith(\"<\") or t[0].endswith(\">\")) and \"->\" in t[0]\n][:10]\nbpe_tokens = [\n    t\n    for t in most_common\n    if (not t[0].startswith(\"<\") and not t[0].endswith(\">\")) and \"->\" not in t[0]\n][:10]\n\n\nast_tokens, bpe_tokens\n\n([('<module -> comment>', 256136),\n  ('<import_from_statement -> from>', 84241),\n  ('<import_statement -> import>', 54511),\n  ('<expression_statement -> string>', 48463),\n  ('<class_definition -> class>', 13860),\n  ('<attribute -> identifier>', 11733),\n  ('<assignment -> identifier>', 11627),\n  ('<future_import_statement -> from>', 8235),\n  ('<argument_list -> string>', 5878),\n  ('<call -> identifier>', 4519)],\n [('.', 6405),\n  ('_', 6387),\n  ('\\n', 3728),\n  ('(', 3227),\n  (',', 3227),\n  ('\\n   ', 2382),\n  ('\\n       ', 2358),\n  (' =', 2073),\n  ('0', 1942),\n  ('1', 1914)])\n\n\n\nast_crosses = [cross_dist[token] for token, _ in ast_tokens]\nbpe_crosses = [cross_dist[token] for token, _ in bpe_tokens]\n\n\nvisualize_perplexities(ast_crosses, ast_tokens, \"AST Tokens\", \"ast_tokens.png\")\n\n\n\n\n\nvisualize_perplexities(bpe_crosses, bpe_tokens, \"BPE Tokens\", \"bpe_tokens.png\")\n\n\n\n\n\nfrom code_tokenizers.core import CodeTokenizer\nfrom transformers import AutoModelForCausalLM\n\nmodel_name = \"bigcode/santacoder\"\npy_tokenizer = CodeTokenizer.from_pretrained(model_name, \"python\", padding_token=\"<|endoftext|>\")\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\npy_tokenizer.tokenizer\n\nExplicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\nExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n\n\nPreTrainedTokenizerFast(name_or_path='bigcode/santacoder', vocab_size=49152, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<fim-prefix>', '<fim-middle>', '<fim-suffix>', '<fim-pad>']})\n\n\n\nast_tokens_least = [\n    t\n    for t in most_common\n    if (t[0].startswith(\"<\") or t[0].endswith(\">\")) and \"->\" in t[0]\n][::-1][:10]\nbpe_tokens_least = [\n    t\n    for t in most_common\n    if (not t[0].startswith(\"<\") and not t[0].endswith(\">\")) and \"->\" not in t[0]\n][::-1][:10]\n\n\nast_crosses_least = [cross_dist[token] for token, _ in ast_tokens_least]\nbpe_crosses_least = [cross_dist[token] for token, _ in bpe_tokens_least]\n\n\nworst_perform = mean_crosses.most_common(10)\nworst_perform\n\nExplicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\nExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n\n\n[('Sticker', 19.77587127685547),\n (' Framework', 18.601856231689453),\n ('proved', 15.978879928588867),\n ('ril', 15.792886734008789),\n ('BM', 15.57440185546875),\n ('PyTorch', 15.425578117370605),\n ('hore', 15.266545295715332),\n ('transforms', 15.242853164672852),\n (\" '{{\", 14.778332710266113),\n (' {?', 14.725968360900879)]"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nloss_func\n\n loss_func (logits, labels)\n\nCalculates the cross entropy loss for the modelâs output and the labels.\n\n\n\n\nDetails\n\n\n\n\nlogits\nthe modelâs output\n\n\nlabels\nthe labels to calculate the cross entropy loss against\n\n\n\n\n# test loss function\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ninputs = tokenizer(\n    [\"Hello, my dog is cute\", \"Hello, my dog is cute\"], return_tensors=\"pt\"\n)\noutputs = model(**inputs)\nlogits = outputs.logits\nlabels = inputs.input_ids\nloss_func(logits, labels)\n\ntensor([[2.3432, 3.7964, 6.6038, 1.7265, 5.4809],\n        [2.3432, 3.7964, 6.6038, 1.7265, 5.4809]], grad_fn=<ViewBackward0>)\n\n\n\nsource\n\n\nget_counts\n\n get_counts (model, tokenizer, batch, semantic_column:str,\n             stop_word_column:str, return_distributions:bool)\n\nReturns the counts for the losses and tokens.\n\n\n\n\nType\nDetails\n\n\n\n\nmodel\n\nthe model to use for predictions\n\n\ntokenizer\n\nthe tokenizer to use for encoding\n\n\nbatch\n\nthe batch to use for predictions\n\n\nsemantic_column\nstr\nthe column to use for semantic predictions\n\n\nstop_word_column\nstr\nthe column to use for stop word predictions\n\n\nreturn_distributions\nbool\nwhether to return the distributions\n\n\n\n\nsource\n\n\nperplexed\n\n perplexed (model:transformers.modeling_utils.PreTrainedModel,\n            dataset:datasets.arrow_dataset.Dataset, tokenizer:transformers\n            .tokenization_utils.PreTrainedTokenizer=None,\n            column:str='text', semantic_column:str=None,\n            stop_word_column:str=None, n_gram:int=1, batch_size:int=1,\n            num_proc:int=2, device:str='cuda', collate_fn=<function\n            default_data_collator>, pass_row:bool=False,\n            return_tokens:bool=False, return_distributions:bool=False,\n            compute_perplexity:bool=True)\n\nCalculate the perplexity of a model on a dataset.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nPreTrainedModel\n\nThe model to calculate the perplexity of.\n\n\ndataset\nDataset\n\nThe dataset to calculate the perplexity on.\n\n\ntokenizer\nPreTrainedTokenizer\nNone\nThe tokenizer to use to tokenize the dataset. If not provided, the tokenizer associated with the model will be used.\n\n\ncolumn\nstr\ntext\nThe column of the dataset to calculate the perplexity on.\n\n\nsemantic_column\nstr\nNone\nThe column of the dataset to calculate the semantic perplexity on such as NER tags.\n\n\nstop_word_column\nstr\nNone\nThe column of the dataset that contains boolean values indicating whether the token is a stop word.\n\n\nn_gram\nint\n1\nThe n-gram to calculate the perplexity on.\n\n\nbatch_size\nint\n1\nThe batch size to use when calculating the perplexity.\n\n\nnum_proc\nint\n2\nThe number of processes to use when tokenizing the dataset.\n\n\ndevice\nstr\ncuda\nThe device to use when calculating the perplexity.\n\n\ncollate_fn\nfunction\ndefault_data_collator\nThe collate function to use when calculating the perplexity.\n\n\npass_row\nbool\nFalse\nWhether to pass the row to the tokenizer.\n\n\nreturn_tokens\nbool\nFalse\nWhether to return the tokens counts along with the perplexity.\n\n\nreturn_distributions\nbool\nFalse\nWhether to return the perplexity distributions instead of the perplexity.\n\n\ncompute_perplexity\nbool\nTrue\nWhether to compute the perplexity. If False, the cross entropy will be returned instead.\n\n\n\n\n\nPerplexity per token\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\nmodel.to(device)\n\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\").select(range(50))\n# filter out empty strings\ndataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n\nperplexity_cnt, token_cnt = perplexed(\n    model,\n    dataset,\n    tokenizer=tokenizer,\n    column=\"text\",\n    batch_size=1,\n    device=device,\n    num_proc=1,\n    return_tokens=True,\n)\nassert len(perplexity_cnt) == len(token_cnt)\nassert perplexity_cnt.keys() == token_cnt.keys()\n\n\n\n\n\n\n\n\n\n\n\ncross_cnt, token_cnt = perplexed(\n    model,\n    dataset,\n    tokenizer=tokenizer,\n    column=\"text\",\n    batch_size=1,\n    device=device,\n    num_proc=1,\n    return_tokens=True,\n    compute_perplexity=False,\n)\nassert len(cross_cnt) == len(token_cnt)\nassert cross_cnt.keys() == token_cnt.keys()\n\n\n\n\n\n\n\n\n\n\n\ncross_cnt.most_common(10)\n\n[(' wired', 17.92612648010254),\n (' shatter', 16.32363510131836),\n (' Career', 15.21772575378418),\n (' Early', 14.70047664642334),\n (' Television', 14.659582138061523),\n (' Daylight', 14.56997299194336),\n (' unrecogn', 14.364179611206055),\n (' @', 14.307954322208058),\n (' Chou', 14.180266380310059),\n (' advisers', 13.927596092224121)]\n\n\n\ncross_cnt.most_common()[-10:]\n\n[('mers', 0.03539723251014948),\n ('mith', 0.018193976022303104),\n ('t', 0.016906073316931725),\n (' than', 0.009314415045082569),\n ('jiang', 0.005416479427367449),\n ('ian', 0.004262291360646486),\n ('aire', 0.002999095479026437),\n ('el', 0.0017088347813114524),\n ('ights', 0.001490435330197215),\n ('sworth', 0.0009158230968751013)]\n\n\n\n# cross entropy of the most common tokens\ntokens = [token for token, _ in token_cnt.most_common(10)]\nfor token in tokens:\n    print(f\"'{token}': {cross_cnt[token]}\")\n\n'<|endoftext|>': 10.327683209001043\n' the': 1.5023754525995046\n',': 2.799564078589466\n'.': 2.2654987903962653\n' \"': 2.2530801612883806\n' in': 2.0132113315057065\n' of': 1.2379778898500193\n' a': 2.107695746828209\n' =': 3.9336307379530697\n' and': 1.6605487003922463\n\n\n\n\nPerplexity per semantic type\nThe following cells contain the code for calculating the perplexity per semantic type of a tokenizer for aligning the AST of a program with the BPE of a language modelâs tokenizer.\n\n!pip install -U code_tokenizers\n!download_grammars\n\n\nfrom code_tokenizers.core import CodeTokenizer\n\ndef code_collator(batch):\n    merged_ast = []\n    for b in batch:\n        merged_ast.append(b.pop(\"merged_ast\"))\n    batch = default_data_collator(batch)\n    batch[\"merged_ast\"] = merged_ast\n    return batch\n\n\nmodel_name = \"codeparrot/codeparrot-small\"\npy_tokenizer = CodeTokenizer.from_pretrained(model_name, \"python\")\npy_tokenizer.tokenizer.pad_token = py_tokenizer.tokenizer.eos_token\npy_tokenizer.pad_token = py_tokenizer.tokenizer.pad_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.to(device)\n\ndataset = load_dataset(\"codeparrot/codeparrot-clean-valid\", split=\"train\").select(\n    range(15)\n)\ncross_cnt, token_cnt = perplexed(\n    model,\n    dataset,\n    tokenizer=py_tokenizer,\n    column=\"content\",\n    semantic_column=\"merged_ast\",\n    stop_word_column=\"is_builtins\",\n    batch_size=1,\n    num_proc=1,\n    device=device,\n    collate_fn=code_collator,\n    return_tokens=True,\n    compute_perplexity=False,\n)\n\nassert len(cross_cnt) == len(token_cnt)\nassert cross_cnt.keys() == token_cnt.keys()\n\n\n\n\n\n\n\n\n\n\n\ncross_cnt.most_common(10)\n\n[('reports', 15.318881034851074),\n ('Double', 15.236268043518066),\n ('BLANK', 15.137480735778809),\n ('148', 14.469829559326172),\n ('BD', 13.819499969482422),\n ('year', 13.65689468383789),\n (' filesystem', 13.625283241271973),\n ('CO', 13.59871768951416),\n ('Pure', 13.172009468078613),\n ('customize', 13.098344802856445)]\n\n\n\ntoken_cnt.most_common(10)\n\n[('<|endoftext|>', 3951),\n ('<module -> comment>', 1479),\n ('< N/A >', 1123),\n ('<attribute -> identifier>', 1019),\n ('<argument_list -> string>', 728),\n ('<expression_statement -> string>', 677),\n ('.', 608),\n ('<dotted_name -> identifier>', 608),\n ('_', 434),\n ('\\n', 391)]\n\n\n\n# perplexity of the most common tokens\ntokens = [token for token, _ in token_cnt.most_common(10)]\nfor token in tokens:\n    print(f\"'{token}': {perplexity_cnt[token]}\")\n\n'<|endoftext|>': 30567.21875\n'<module -> comment>': 0\n'< N/A >': 0\n'<attribute -> identifier>': 0\n'<argument_list -> string>': 0\n'<expression_statement -> string>': 0\n'.': 9.635930061340332\n'<dotted_name -> identifier>': 0\n'_': 0\n'\n': 3.0456223487854004"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "perplexed",
    "section": "",
    "text": "This library is based on the idea from Andrej Karpathy on understanding the failure cases of a model by looking at the worst predictions. Specifically, this library focuses on calculating the perplexity of Large Language Models (LLMs) such as GPT-2 and BERT. The idea is to calculate the perplexity of a model on a dataset at the per token level. This allows us to understand where the model is perplexed and where it is not. This is useful for debugging and understanding the model."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "perplexed",
    "section": "Install",
    "text": "Install\npip install perplexed"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "perplexed",
    "section": "How to use",
    "text": "How to use\n\nUsing the API\nperplexed is designed to work with the HuggingFace ecosystem and is built on top of the transformers and datasets libraries. The API is designed to be simple and easy to use. The main function is perplexed which takes in a model, dataset, and tokenizer and returns a simple Counter object with the perplexity of each token in the dataset. Here is an example of how to use it:\n\nfrom perplexed.core import perplexed\n\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\").select(range(100))\n# filter out empty strings\ndataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n\nperplexity_cnt = perplexed(\n    model, dataset, tokenizer=tokenizer, column=\"text\", batch_size=1, device=\"cpu\"\n)\nperplexity_cnt.most_common(10)\n\nFound cached dataset wikitext (/home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-68eb731029328d8b.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1c1cd85efcee4db8.arrow\n\n\n[(' wired', 60983688.0),\n (' 768', 21569838.0),\n (' shatter', 12281687.0),\n (' unsett', 8289435.0),\n (' ignited', 6605209.0),\n (' Tanz', 4834899.0),\n (' Influence', 4153321.75),\n (' Career', 4064189.0),\n (' Television', 2325870.75),\n (' Moral', 2243574.5)]"
  },
  {
    "objectID": "tutorial_eda.html",
    "href": "tutorial_eda.html",
    "title": "Tutorial - Exploratory Data Analysis",
    "section": "",
    "text": "This notebook is a tutorial on how to use the perplexed library to perform exploratory data analysis (EDA) on a dataset. It is intended to be a quick introduction to the library, and is not intended to be a comprehensive guide to the library.\nLetâs get some data to work with. Weâll use the wikitext dataset from the datasets library.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\").select(\n    range(10)\n)\ndataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n\n/home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nFound cached dataset wikitext (/home/nathan/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-ceb6bd7c8619d4cb.arrow\n\n\nNow letâs find a model to use. Weâll use the EleutherAI/gpt-neo-125M model from the transformers library.\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n\nNow that all of that is done, we can start using perplexed to perform EDA on the dataset. Letâs run our model on the dataset and get the perplexity of each sample.\n\nfrom perplexed.core import perplexed\n\nperplexity_cnt, token_cnt = perplexed(\n    model,\n    dataset,\n    tokenizer=tokenizer,\n    column=\"text\",\n    batch_size=1,\n    device=\"cpu\",\n    return_tokens=True,\n)\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-35d4a307752a997a.arrow\n\n\nThe perplexed function returns two values: a Counter object containing the perplexity of each token, and the total number of tokens in the dataset. We can now explore things like what the most perplexed tokens are:\n\nperplexity_cnt.most_common(10)\n\n[(' unavoid', 6007780352.0),\n (' Explosive', 188573856.0),\n (' wired', 60983452.0),\n (' 768', 21569920.0),\n (' ridden', 15362581.0),\n (' Influ', 15165454.0),\n (' esche', 14969172.0),\n (' reluctant', 14123441.0),\n (' succumb', 12422194.0),\n (' Arm', 12384815.0)]\n\n\nYou can also look at what the perplexity is for the most common tokens:\n\ntokens, _ = zip(*token_cnt.most_common(10))\n# show the perplexity for the most common tokens\nfor token in tokens:\n    print(token, perplexity_cnt[token])\n\n the 3.5897090435028076\n, 21.938007354736328\n. 10.295743942260742\n of 2.934633731842041\n and 9.320274353027344\n in 10.81938648223877\n to 4.0181355476379395\n a 10.546797752380371\n @ 10436.27734375\n@ 29.87317657470703\n\n\nOr you can do the least common tokens:\n\ntokens, _ = zip(*token_cnt.most_common()[-10:])\n# show the perplexity for the least common tokens\nfor token in tokens:\n    print(token, perplexity_cnt[token])\n\n designation 18.893896102905273\n 99 4433.51416015625\n Much 3717.565185546875\n urban 15859.9248046875\n residential 4258.7236328125\n businesses 3.116973638534546\n rural 691.44921875\n Rural 65607.2265625\n Cemetery 24.99341583251953\n terminating 93.44629669189453\n\n\nNow if you want to look at the distribution of perplexities, you can do that too!\n\nperplexity_dist, token_cnt = perplexed(\n    model,\n    dataset,\n    tokenizer=tokenizer,\n    column=\"text\",\n    batch_size=1,\n    device=\"cpu\",\n    return_tokens=True,\n    return_distributions=True,\n)\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-35d4a307752a997a.arrow\n\n\nNow instead of a Counter object, perplexed returns a dictionary mapping each token to the different perplexities it has. So, if we are so inclined, we could make a boxplot of the perplexities for each token:\n\n# boxplot the distribution of perplexities for the most common tokens\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\nmost_common = token_cnt.most_common(15)\nmost_common_tokens = [token for token, _ in most_common]\nmost_common_perplexities = [\n    list(filter(lambda x: x < 10, perplexity_dist[token]))\n    for token in most_common_tokens\n]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax = sns.boxplot(data=most_common_perplexities, palette=\"Set2\")\nax.set_xticklabels(most_common_tokens)\nax.set_title(\"Perplexity Distribution for the Most Common Tokens\")\nplt.show()"
  },
  {
    "objectID": "tutorial_code.html",
    "href": "tutorial_code.html",
    "title": "Tutorial - Code Analysis",
    "section": "",
    "text": "<<<<<<< HEAD\n=======\n>>>>>>> da2631218c3ce46078ebd2300a91557e85df83bc\n<<<<<<< HEAD\n\n!pip install -U git+https://github.com/ncoop57/code_tokenizers.git\n!download_grammars\n\nCollecting git+https://github.com/ncoop57/code_tokenizers.git\n  Cloning https://github.com/ncoop57/code_tokenizers.git to /tmp/pip-req-build-vjimbq4_\n  Running command git clone --filter=blob:none --quiet https://github.com/ncoop57/code_tokenizers.git /tmp/pip-req-build-vjimbq4_\n  Resolved https://github.com/ncoop57/code_tokenizers.git to commit cdd8368b4ac26ee8bc4653664786224da52bbb46\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: fastcore in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code-tokenizers==0.0.4) (1.5.27)\nRequirement already satisfied: gitpython in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code-tokenizers==0.0.4) (3.1.29)\nRequirement already satisfied: pandas in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code-tokenizers==0.0.4) (1.5.1)\nRequirement already satisfied: transformers in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code-tokenizers==0.0.4) (4.24.0)\nRequirement already satisfied: tree-sitter==0.20.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code-tokenizers==0.0.4) (0.20.1)\nRequirement already satisfied: pip in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from fastcore->code-tokenizers==0.0.4) (22.2.2)\nRequirement already satisfied: packaging in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from fastcore->code-tokenizers==0.0.4) (21.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from gitpython->code-tokenizers==0.0.4) (4.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from pandas->code-tokenizers==0.0.4) (2.8.2)\nRequirement already satisfied: numpy>=1.21.0 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from pandas->code-tokenizers==0.0.4) (1.23.5)\nRequirement already satisfied: pytz>=2020.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from pandas->code-tokenizers==0.0.4) (2022.6)\nRequirement already satisfied: filelock in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code-tokenizers==0.0.4) (3.8.0)\nRequirement already satisfied: pyyaml>=5.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code-tokenizers==0.0.4) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code-tokenizers==0.0.4) (4.64.1)\nRequirement already satisfied: regex!=2019.12.17 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code-tokenizers==0.0.4) (2022.10.31)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code-tokenizers==0.0.4) (0.13.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code-tokenizers==0.0.4) (0.11.0)\nRequirement already satisfied: requests in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code-tokenizers==0.0.4) (2.28.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->code-tokenizers==0.0.4) (5.0.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers->code-tokenizers==0.0.4) (4.4.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from packaging->fastcore->code-tokenizers==0.0.4) (3.0.9)\nRequirement already satisfied: six>=1.5 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->code-tokenizers==0.0.4) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code-tokenizers==0.0.4) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code-tokenizers==0.0.4) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code-tokenizers==0.0.4) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code-tokenizers==0.0.4) (2022.9.24)\n\n\n\nfrom datasets import load_dataset\n\n# dataset = load_dataset(\n#   \"codeparrot/github-code\",\n#   split=\"train\",\n#   streaming=True,\n#   languages=[\"Python\"],\n#   licenses=[\"gpl-3.0\"],\n# )\nds = load_dataset(\n    \"bigcode/the-stack-smol\", data_dir=\"data/python\", split=\"train\"\n).select(range(5_000))\nfiltered_ds = ds.filter(lambda example: len(example[\"content\"]) < 4096)\nlen(filtered_ds)\n\nUsing custom data configuration bigcode--the-stack-smol-7b51f8bde3058781\nFound cached dataset json (/home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-b38dcc32a872398b.arrow\n\n\n3072\n\n\n\ndef find_duplicates(items):\n    # Create an empty set to store the items that we have already seen\n    seen = set()\n\n    # Create an empty list to store the duplicates that we find\n    duplicates = []\n\n    # Loop through each item in the list\n    for item in items:\n        # If the item is already in the \"seen\" set, then it must be a duplicate\n        if item in seen:\n            # Add the duplicate to the list\n            duplicates.append(item)\n        # If the item is not in the \"seen\" set, then add it to the set\n        else:\n            seen.add(item)\n\n    # Return the list of duplicates\n    return duplicates\n\n\nrepo_names = find_duplicates(filtered_ds[\"repository_name\"])\n\nrepo_files = {}\nfor repo_name in repo_names:\n    rows_w_repo = filtered_ds.filter(\n        lambda example: example[\"repository_name\"] == repo_name\n    )\n\n    if len(rows_w_repo) > 1:\n        repo_files[repo_name] = [row[\"content\"] for row in rows_w_repo]\n        if len(repo_files) > 400:\n            break\n\n# filter out repos with only one file\nfiltered_ds = filtered_ds.filter(\n    lambda example: example[\"repository_name\"] in repo_files\n)\nlen(filtered_ds)\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0dd594043cdb7dfb.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9e8d9f11675f0fd5.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-617ac77a84286d46.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6477901bac5699a3.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-8fba30c0043e2758.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-b9494a3148fba70c.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-df87f5fa053f298d.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-3cb8409985b9c937.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6249181d8b91acb7.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-7e02fa7394d5562d.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9e8d9f11675f0fd5.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-21a1aed9500babb7.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-e4c090ff2a34e9f1.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fe53bd60d0b0af09.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-78ad9175d54e6807.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-a93eee68fd7bf113.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-78ad9175d54e6807.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-a0eb259f7ed83dca.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-bc011177ae592d36.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6c83726c0482208d.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-81a33b25fc2015a0.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-42b767aa1258b99c.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5a1a7bbf68bb6f47.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f438698e6d86dc9b.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-cfa703d0b23aced3.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5b607ae7843b956f.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0dd594043cdb7dfb.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f89e7a6a0690390e.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5fa1e2cf9bec70a2.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-21a1aed9500babb7.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-cfa703d0b23aced3.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f89e7a6a0690390e.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-2b0a34b49811dbf1.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9e8d9f11675f0fd5.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6e433f135cafc815.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-44447d1e1ca823c3.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0d517112acbcb2fb.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f438698e6d86dc9b.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f89e7a6a0690390e.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-af7d990b4c5a25a8.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9e8d9f11675f0fd5.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-e5233d545d28d276.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-7377b8cd9c22fd1d.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f3e4160a738fa4e9.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6c3cf16ede15d46b.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-93435b3794c4f665.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5fd9f1b02f803632.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-56653a929f870d45.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-931d2a0e669bfa84.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-012ff68365940ba9.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0c4e88dc3adb011e.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-eef09789656be2c2.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f89e7a6a0690390e.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-7dd15631ba19a05f.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f948533773af0187.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-cfa703d0b23aced3.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f89e7a6a0690390e.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-03c18ba23c058f0c.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f22fb3e7678f3470.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-2c83b3ee0341bb08.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-bc011177ae592d36.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-bbca953310dabd27.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-038e7b5b9f18149d.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-859475b190db3748.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0bb22f71b25c52ea.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-372efa95fb14c1f5.arrow\n\n\n125\n\n\n\nfrom code_tokenizers.core import CodeTokenizer\nfrom transformers import AutoModelForCausalLM\n\nmodel_name = \"codeparrot/codeparrot-small\"\npy_tokenizer = CodeTokenizer.from_pretrained(\n    model_name, \"python\", padding_token=\"<|endoftext|>\"\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\npy_tokenizer.tokenizer\n\nPreTrainedTokenizerFast(name_or_path='codeparrot/codeparrot-small', vocab_size=32768, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})\n\n\n\nfrom code_tokenizers.helpers import get_internal_methods\n\n# add the internal methods to the dataset\nfiltered_ds = filtered_ds.map(\n    lambda example: {\n        \"internal_methods\": get_internal_methods(\n            repo_files[example[\"repository_name\"]], py_tokenizer\n        )\n    }\n)\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-71841d6a862bf898.arrow\n\n\n\nfrom functools import partial\nfrom transformers import default_data_collator\n\n\ndef code_collator(batch):\n    merged_ast = []\n    for b in batch:\n        merged_ast.append(b.pop(\"merged_ast\"))\n\n    batch = default_data_collator(batch)\n    batch[\"merged_ast\"] = merged_ast\n    return batch\n\n\ndef tokenizer_wrapper(tokenizer, example, column, *args, **kwargs):\n    return tokenizer(\n        example[column], internal_methods=example[\"internal_methods\"], *args, **kwargs\n    )\n\n\n# Setup tokenizer\ntokenizer = partial(tokenizer_wrapper, py_tokenizer, column=\"content\")\ntokenizer.decode = py_tokenizer.decode\n\n\nperplexity_cnt, token_cnt = perplexed(\n    model,\n    filtered_ds,\n    tokenizer=tokenizer,\n    column=\"content\",\n    semantic_column=\"merged_ast\",\n    batch_size=1,\n    num_proc=32,\n    device=\"cpu\",\n    collate_fn=code_collator,\n    pass_row=True,\n    return_tokens=True,\n    # return_distributions=True,\n)\n\n                                 \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-09ac10f3bc3e9985.arrow\n\n\n  \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-d00979d6c3b04126.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-edd6542c50942511.arrow\n\n\n   \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-8857911761b91855.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-78c57230b30b284e.arrow\n\n\n  \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-396c588294c0e570.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-83fa61f956b994d3.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-d8e745a84188e435.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-867c817d55b28e6e.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-4f9a19be9cd215ae.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-d52392867b4c8124.arrow\n\n\n  \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-3d46918ef0191e2e.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-b9e0a3ac035e8f0a.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-78ffce0c069a9f5c.arrow\n\n\n   \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0be83553c38a389e.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0da3aa2c4082256e.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-ca705bd2f4eda67a.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0f9d670af10f880c.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-e1003885968e6cab.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-e72e29ca2c94f49c.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-8bafc125da2f9c58.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f9fafa3c5bd46ae0.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f6a4df8f610d8100.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-c40d771daf71174e.arrow\n\n\n  \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-d0fed41c1c049eb7.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-39bf84260fe8e79a.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-29395061109142b6.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-3a824fb246389011.arrow\n\n\n  \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9a474997473b9638.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-a72fea5dc2a8feed.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-744ba52de7e111d8.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9431655aa6be6287.arrow\n\n\n\n\n\n\n\n\n\n\n\n\nfrom perplexed.core import perplexed\n\nperplexity_dist, token_cnt = perplexed(\n    model,\n    filtered_ds,\n    tokenizer=tokenizer,\n    column=\"content\",\n    semantic_column=\"merged_ast\",\n    batch_size=1,\n    num_proc=32,\n    device=\"cpu\",\n    collate_fn=code_collator,\n    pass_row=True,\n    return_tokens=True,\n    return_distributions=True,\n)\n\n                                 \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-4f9a19be9cd215ae.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-d00979d6c3b04126.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-8857911761b91855.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-d8e745a84188e435.arrow\n\n\n  \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-78c57230b30b284e.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-83fa61f956b994d3.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-edd6542c50942511.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-867c817d55b28e6e.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0da3aa2c4082256e.arrow\n\n\n  \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-78ffce0c069a9f5c.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-09ac10f3bc3e9985.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-d52392867b4c8124.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-396c588294c0e570.arrow\n\n\n  \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0be83553c38a389e.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-3d46918ef0191e2e.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-e72e29ca2c94f49c.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-ca705bd2f4eda67a.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-b9e0a3ac035e8f0a.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0f9d670af10f880c.arrow\n\n\n  \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-e1003885968e6cab.arrow\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-8bafc125da2f9c58.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f6a4df8f610d8100.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-39bf84260fe8e79a.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-c40d771daf71174e.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-d0fed41c1c049eb7.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-29395061109142b6.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9a474997473b9638.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-744ba52de7e111d8.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-3a824fb246389011.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f9fafa3c5bd46ae0.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-a72fea5dc2a8feed.arrow\n\n\n \n\n\nLoading cached processed dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9431655aa6be6287.arrow\n\n\n\n\n\n\n\n\n\n\n\n\nmost_common = token_cnt.most_common(2_000)\n# filter out that don't start with a < and end with a >\nmost_common = [\n    t\n    for t in most_common\n    if t[0].startswith(\"<argument_list\") or t[0].startswith(\"<call\")\n]  # [:10]\n\n\nmost_common\n\n[('<argument_list -> string>', 3092),\n ('<call -> identifier>', 2009),\n ('<argument_list -> (>', 1527),\n ('<argument_list -> identifier>', 772),\n ('<argument_list -> )>', 648),\n ('<argument_list -> ,>', 453),\n ('<argument_list -> comment>', 346),\n ('<argument_list -> identifier (internal)>', 229),\n ('<call -> identifier (internal)>', 211),\n ('<argument_list -> integer>', 88),\n ('<argument_list -> ( (internal)>', 60),\n ('<argument_list -> ) (internal)>', 28),\n ('<argument_list -> , (internal)>', 23),\n ('<argument_list -> integer (internal)>', 21),\n ('<argument_list -> string (internal)>', 20),\n ('<argument_list -> float>', 17),\n ('<argument_list -> none>', 7)]\n\n\n\n# boxplot the distribution of perplexities for the most common tokens\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n# most_common = token_cnt.most_common(15)\nmost_common_tokens = [token for token, _ in most_common]\nmost_common_perplexities = [\n    list(filter(lambda x: x < 10, perplexity_dist[token]))\n    for token in most_common_tokens\n]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax = sns.boxplot(data=most_common_perplexities, palette=\"Set2\")\nax.set_xticklabels(most_common_tokens)\nax.set_title(\"Perplexity Distribution for the Most Common Tokens\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n\n\n\n\n# most_common = token_cnt.most_common(2_000)\n# filter out that don't start with a < and end with a >\nmost_common = [t for t in token_cnt if \"internal\" in t[0]][:10]\n\n\nmost_common\n\n[]\n\n\n\nplt.clf()\n\n\n# boxplot the distribution of perplexities for the most common tokens\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n# most_common = token_cnt.most_common(15)\nmost_common_tokens = [token for token, _ in most_common]\nmost_common_perplexities = [\n    list(filter(lambda x: x < 10, perplexity_dist[token]))\n    for token in most_common_tokens\n]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax = sns.boxplot(data=most_common_perplexities, palette=\"Set2\")\nax.set_xticklabels(most_common_tokens)\nax.set_title(\"Perplexity Distribution for the Most Common Tokens\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n\nfrom datasets import load_dataset\nfrom tqdm.auto import tqdm\n\nds = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\", split=\"train\")\nfiltered_ds = ds.filter(lambda example: len(example[\"content\"]) < 4096)\nrepo_names = set(filtered_ds[\"repository_name\"])\n\nrepo_files = {}\nfor repo_name in tqdm(repo_names, desc=\"Processing repos\", total=len(repo_names)):\n    rows_w_repo = filtered_ds.filter(\n        lambda example: example[\"repository_name\"] == repo_name\n    )\n\n    if len(rows_w_repo) > 1:\n        repo_files[repo_name] = [row[\"content\"] for row in rows_w_repo]\n        if len(repo_files) > 100:\n            break\n\n=======\n\n# from datasets import load_dataset\n\n# dataset = load_dataset(\n#   \"codeparrot/github-code\",\n#   split=\"train\",\n#   streaming=True,\n#   languages=[\"Python\"],\n#   licenses=[\"gpl-3.0\"],\n# )\n# dataset = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\", split=\"train\").select(range(5_000))\n\n\n# !pip install -U git+https://github.com/ncoop57/code_tokenizers.git\n!download_grammars\n\n\n1 + 1\n\n\nfrom datasets import load_dataset\nfrom tqdm.auto import tqdm\n\nds = load_dataset(\n    \"bigcode/the-stack-smol\", data_dir=\"data/python\", split=\"train\"\n).select(range(5_000))\nfiltered_ds = ds.filter(lambda example: len(example[\"content\"]) < 4096)\n\nUsing custom data configuration bigcode--the-stack-smol-7b51f8bde3058781\nFound cached dataset json (/transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-b38dcc32a872398b.arrow\n\n\n\nlen(filtered_ds)\n\n3072\n\n\n\ndef find_duplicates(items):\n    # Create an empty set to store the items that we have already seen\n    seen = set()\n\n    # Create an empty list to store the duplicates that we find\n    duplicates = []\n\n    # Loop through each item in the list\n    for item in items:\n        # If the item is already in the \"seen\" set, then it must be a duplicate\n        if item in seen:\n            # Add the duplicate to the list\n            duplicates.append(item)\n        # If the item is not in the \"seen\" set, then add it to the set\n        else:\n            seen.add(item)\n\n    # Return the list of duplicates\n    return duplicates\n\n\nrepo_names = find_duplicates(filtered_ds[\"repository_name\"])\n\n\nlen(repo_names)\n\n73\n\n\n\nrepo_files = {}\nfor repo_name in repo_names:\n    rows_w_repo = filtered_ds.filter(\n        lambda example: example[\"repository_name\"] == repo_name\n    )\n\n    if len(rows_w_repo) > 1:\n        repo_files[repo_name] = [row[\"content\"] for row in rows_w_repo]\n        if len(repo_files) > 100:\n            break\n\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0dd594043cdb7dfb.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9e8d9f11675f0fd5.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-617ac77a84286d46.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6477901bac5699a3.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-8fba30c0043e2758.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-b9494a3148fba70c.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-df87f5fa053f298d.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-3cb8409985b9c937.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6249181d8b91acb7.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-7e02fa7394d5562d.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9e8d9f11675f0fd5.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-21a1aed9500babb7.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-e4c090ff2a34e9f1.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fe53bd60d0b0af09.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-78ad9175d54e6807.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-a93eee68fd7bf113.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-78ad9175d54e6807.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-a0eb259f7ed83dca.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-bc011177ae592d36.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6c83726c0482208d.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-81a33b25fc2015a0.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-42b767aa1258b99c.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5a1a7bbf68bb6f47.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f438698e6d86dc9b.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-cfa703d0b23aced3.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5b607ae7843b956f.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0dd594043cdb7dfb.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f89e7a6a0690390e.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5fa1e2cf9bec70a2.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-21a1aed9500babb7.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-cfa703d0b23aced3.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f89e7a6a0690390e.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-2b0a34b49811dbf1.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9e8d9f11675f0fd5.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6e433f135cafc815.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-44447d1e1ca823c3.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0d517112acbcb2fb.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f438698e6d86dc9b.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f89e7a6a0690390e.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-af7d990b4c5a25a8.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-9e8d9f11675f0fd5.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-e5233d545d28d276.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-7377b8cd9c22fd1d.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f3e4160a738fa4e9.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6c3cf16ede15d46b.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-93435b3794c4f665.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5fd9f1b02f803632.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-56653a929f870d45.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-931d2a0e669bfa84.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-012ff68365940ba9.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0c4e88dc3adb011e.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-eef09789656be2c2.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f89e7a6a0690390e.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-7dd15631ba19a05f.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f948533773af0187.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-cfa703d0b23aced3.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f89e7a6a0690390e.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fa7394602e11c848.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-03c18ba23c058f0c.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-f22fb3e7678f3470.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-2c83b3ee0341bb08.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-bc011177ae592d36.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-bbca953310dabd27.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-038e7b5b9f18149d.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-859475b190db3748.arrow\nLoading cached processed dataset at /transformers_cache/bigcode___json/bigcode--the-stack-smol-7b51f8bde3058781/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-0bb22f71b25c52ea.arrow\n\n\n>>>>>>> da2631218c3ce46078ebd2300a91557e85df83bc\n\nlen(filtered_ds)\n\n\n# filter out repos with only one file\nfiltered_ds = filtered_ds.filter(\n    lambda example: example[\"repository_name\"] in repo_files\n)\n\n\nlen(filtered_ds)\n\n<<<<<<< HEAD\n\nfiltered_ds\n\n=======\n>>>>>>> da2631218c3ce46078ebd2300a91557e85df83bc\n\nfrom code_tokenizers.core import CodeTokenizer\nfrom transformers import AutoModelForCausalLM\n\nmodel_name = \"codeparrot/codeparrot-small\"\npy_tokenizer = CodeTokenizer.from_pretrained(\n    model_name, \"python\", padding_token=\"<|endoftext|>\"\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\npy_tokenizer.tokenizer\n\n<<<<<<< HEAD\n\nfrom code_tokenizers.helpers import get_internal_methods\n\ninternal_methods = get_internal_methods(\n    repo_files[\"reduceus/connect-python-sdk\"], py_tokenizer\n)\ninternal_methods\n\n=======\n\nfrom code_tokenizers.helpers import get_internal_methods\n\n# internal_methods = get_internal_methods(repo_files[\"reduceus/connect-python-sdk\"], py_tokenizer)\n# internal_methods\n\n>>>>>>> da2631218c3ce46078ebd2300a91557e85df83bc\n\n# add the internal methods to the dataset\nfiltered_ds = filtered_ds.map(\n    lambda example: {\n        \"internal_methods\": get_internal_methods(\n            repo_files[example[\"repository_name\"]], py_tokenizer\n        )\n    }\n)\n\n\nfiltered_ds[1]\n\n\nfrom transformers import default_data_collator\n\n\ndef code_collator(batch):\n    merged_ast = []\n    for b in batch:\n        merged_ast.append(b.pop(\"merged_ast\"))\n\n    batch = default_data_collator(batch)\n    batch[\"merged_ast\"] = merged_ast\n    return batch\n\n<<<<<<< HEAD\n\nfrom functools import partial\n\n\ndef tokenizer_wrapper(tokenizer, example, column, *args, **kwargs):\n    return tokenizer(\n        example[column], internal_methods=example[\"internal_methods\"], *args, **kwargs\n    )\n    # return tokenizer(example[\"content\"])\n\n\ntokenizer = partial(tokenizer_wrapper, py_tokenizer, column=\"content\")\n\n=======\n\nfrom functools import partial\n\n\ndef tokenizer_wrapper(tokenizer, example, column, *args, **kwargs):\n    # print(example[\"internal_methods\"])\n    return tokenizer(\n        example[column], internal_methods=example[\"internal_methods\"], *args, **kwargs\n    )\n    # return tokenizer(example[\"content\"])\n\n\ntokenizer = partial(tokenizer_wrapper, py_tokenizer, column=\"content\")\n\n>>>>>>> da2631218c3ce46078ebd2300a91557e85df83bc\n\ntokenizer.decode = py_tokenizer.decode\n\n<<<<<<< HEAD\n\nfrom perplexed.core import perplexed\n\nperplexity_dist, token_cnt = perplexed(\n    model,\n    filtered_ds,\n    tokenizer=tokenizer,\n    column=\"content\",\n    semantic_column=\"merged_ast\",\n    batch_size=10,\n    num_proc=4,\n    device=\"cpu\",\n    collate_fn=code_collator,\n    pass_row=True,\n    return_tokens=True,\n    return_distributions=True,\n)\n\n\nmost_common = token_cnt.most_common(2_000)\n# filter out that don't start with a < and end with a >\nmost_common = [t for t in most_common if t[0].startswith(\"<\") and t[0].endswith(\">\")]\n\n\ntoken_cnt\n\n\n[t for t in token_cnt if \"internal\" in t[0]]\n\n=======\n\nfrom perplexed.core import perplexed\n\nperplexity_dist, token_cnt = perplexed(\n    model,\n    filtered_ds,\n    tokenizer=tokenizer,\n    column=\"content\",\n    semantic_column=\"merged_ast\",\n    batch_size=1,\n    num_proc=4,\n    device=\"cpu\",\n    collate_fn=code_collator,\n    pass_row=True,\n    return_tokens=True,\n    return_distributions=True,\n)\n\n       \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmost_common = token_cnt.most_common(2_000)\n# filter out that don't start with a < and end with a >\n# most_common = [t for t in most_common if t[0].startswith(\"<\") and t[0].endswith(\">\")]\n\n\nmost_common = [\n    t\n    for t in most_common\n    if \"<call -> identifier (internal)>\" in t[0] or \"<call -> identifier>\" in t[0]\n]\n\n>>>>>>> da2631218c3ce46078ebd2300a91557e85df83bc\n\nmost_common\n\n<<<<<<< HEAD\n\n# boxplot the distribution of perplexities for the most common tokens\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n# most_common = token_cnt.most_common(15)\nmost_common_tokens = [token for token, _ in most_common]\nmost_common_perplexities = [\n    list(filter(lambda x: x < 10, perplexity_dist[token]))\n    for token in most_common_tokens\n]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax = sns.boxplot(data=most_common_perplexities, palette=\"Set2\")\nax.set_xticklabels(most_common_tokens)\nax.set_title(\"Perplexity Distribution for the Most Common Tokens\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n=======\n\n# boxplot the distribution of perplexities for the most common tokens\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n# most_common = token_cnt.most_common(15)\nmost_common_tokens = [token for token, _ in most_common]\nmost_common_perplexities = [\n    list(filter(lambda x: x < 1.2, perplexity_dist[token]))\n    for token in most_common_tokens\n]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax = sns.boxplot(data=most_common_perplexities, palette=\"Set2\")\nax.set_xticklabels(most_common_tokens)\nax.set_title(\"Perplexity Distribution for the Most Common Tokens\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n\n\n\n>>>>>>> da2631218c3ce46078ebd2300a91557e85df83bc"
  }
]