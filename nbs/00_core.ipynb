{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from collections import Counter\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datasets import load_dataset\n",
    "from nbdev.showdoc import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def loss_func(logits, labels):\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_counts(model, tokenizer, batch, semantic_column):\n",
    "    input_ids = torch.tensor(batch[\"input_ids\"])\n",
    "    attention_mask = torch.tensor(batch[\"attention_mask\"])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids, return_dict=True)\n",
    "    loss = loss_func(outputs.logits, input_ids)\n",
    "    # Add the losses to the counter for each \n",
    "    # token in the input\n",
    "    loss_cnt = Counter()\n",
    "    token_cnt = Counter()\n",
    "    for i, token in enumerate(input_ids[1:]): # Skip the first token since labels are shifted\n",
    "        token = tokenizer.decode(token)\n",
    "        loss_cnt[token] += loss[i].item()\n",
    "        token_cnt[token] += 1\n",
    "    \n",
    "        if semantic_column != None:\n",
    "            semantic = batch[semantic_column][i]\n",
    "            loss_cnt[semantic] += loss[i].item()\n",
    "            token_cnt[semantic] += 1\n",
    "    return loss_cnt, token_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def perplexed(\n",
    "    model: transformers.PreTrainedModel, # The model to calculate the perplexity of.\n",
    "    dataset: datasets.Dataset, # The dataset to calculate the perplexity on.\n",
    "    tokenizer: transformers.PreTrainedTokenizer = None, # The tokenizer to use to tokenize the dataset. If not provided, the tokenizer associated with the model will be used.\n",
    "    column: str = \"text\", # The column of the dataset to calculate the perplexity on.\n",
    "    semantic_column: str = None, # The column of the dataset to calculate the semantic perplexity on such as NER tags.\n",
    "    n_gram: int = 1, # The n-gram to calculate the perplexity on.\n",
    "    batch_size: int = 1, # The batch size to use when calculating the perplexity.\n",
    "    device: str = \"cuda\", # The device to use when calculating the perplexity.\n",
    "    return_tokens: bool = False, # Whether to return the tokens counts along with the perplexity.\n",
    "): # The perplexity of the model on the dataset or a tuple of the perplexity and the token counts.\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a model on a dataset.\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = model.config.tokenizer_class.from_pretrained(model.config.pretrained_model_name_or_path)\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenizer(x[column], return_tensors=\"pt\", truncation=True),\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    # TODO: Add support for semantic perplexity\n",
    "\n",
    "    # Calculate the perplexity of the model on the dataset\n",
    "    total_loss_cnt = Counter()\n",
    "    total_token_cnt = Counter()\n",
    "    for batch in tokenized_dataset:\n",
    "        # calculate the loss for each token\n",
    "        loss_cnt, token_cnt = get_counts(model, tokenizer, batch, semantic_column)\n",
    "        # add the loss and token counts to the total\n",
    "        total_loss_cnt += loss_cnt\n",
    "        total_token_cnt += token_cnt\n",
    "    \n",
    "    # Calculate the perplexity\n",
    "    perplexity_cnt = Counter()\n",
    "    for token, loss in total_loss_cnt.items():\n",
    "        perplexity_cnt[token] = torch.exp(torch.tensor(loss / total_token_cnt[token])).item()\n",
    "    \n",
    "    if return_tokens:\n",
    "        return perplexity_cnt, total_token_cnt\n",
    "    \n",
    "    return perplexity_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\").select(range(1_000))\n",
    "# filter out empty strings\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "perplexity_cnt, token_cnt = perplexed(model, dataset, tokenizer=tokenizer, column=\"text\", batch_size=1, device=\"cpu\", return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, _ = zip(*perplexity_cnt.most_common(10))\n",
    "# show the token counts for the most perplexed tokens\n",
    "for token in tokens:\n",
    "    print(token, token_cnt[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top least common tokens\n",
    "perplexity_cnt.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, _ = zip(*perplexity_cnt.most_common()[-10:])\n",
    "# show the token counts for the least perplexed tokens\n",
    "for token in tokens:\n",
    "    print(token, token_cnt[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common token\n",
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, _ = zip(*token_cnt.most_common(10))\n",
    "# show the perplexity for the most common tokens\n",
    "for token in tokens:\n",
    "    print(token, perplexity_cnt[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the least common token\n",
    "token_cnt.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, _ = zip(*token_cnt.most_common()[-10:])\n",
    "# show the perplexity for the least common tokens\n",
    "for token in tokens:\n",
    "    print(token, perplexity_cnt[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('perplexed')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
