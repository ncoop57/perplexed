{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> This module contains the core functions for calculating the perplexity of a language model per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from rich.progress import track\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datasets import load_dataset\n",
    "from nbdev.showdoc import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def loss_func(\n",
    "    logits, # the model's output\n",
    "    labels  # the labels to calculate the cross entropy loss against\n",
    "):          # the loss per token of shape (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    Calculates the cross entropy loss for the model's output and the labels.\n",
    "    \"\"\"\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # reshape to (batch_size, sequence_length)\n",
    "    loss = loss.view(*shift_labels.size())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_counts(\n",
    "    model,                      # the model to use for predictions\n",
    "    tokenizer,                  # the tokenizer to use for encoding\n",
    "    batch,                      # the batch to use for predictions\n",
    "    semantic_column: str,       # the column to use for semantic predictions\n",
    "    return_distributions: bool  # whether to return the distributions\n",
    "):                              # the counts for the losses and tokens\n",
    "    \"\"\"\n",
    "    Returns the counts for the losses and tokens.\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids, return_dict=True)\n",
    "    loss = loss_func(outputs.logits, input_ids)\n",
    "\n",
    "    # Add the losses to the counter for each \n",
    "    # token in the input\n",
    "    loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    token_cnt = Counter()\n",
    "    for i, ids in enumerate(input_ids):\n",
    "        for j, token in enumerate(ids[1:]):\n",
    "            token = tokenizer.decode(token)\n",
    "            loss_cnt[token] += [loss[i][j].item()] if return_distributions else loss[i][j].item()\n",
    "            token_cnt[token] += 1\n",
    "\n",
    "            if semantic_column != None:\n",
    "                semantic = batch[semantic_column][i][j]\n",
    "                loss_cnt[semantic] += [loss[i][j].item()] if return_distributions else loss[i][j].item()\n",
    "                token_cnt[semantic] += 1\n",
    "\n",
    "    return loss_cnt, token_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def perplexed(\n",
    "    model: transformers.PreTrainedModel, # The model to calculate the perplexity of.\n",
    "    dataset: datasets.Dataset, # The dataset to calculate the perplexity on.\n",
    "    tokenizer: transformers.PreTrainedTokenizer = None, # The tokenizer to use to tokenize the dataset. If not provided, the tokenizer associated with the model will be used.\n",
    "    column: str = \"text\", # The column of the dataset to calculate the perplexity on.\n",
    "    semantic_column: str = None, # The column of the dataset to calculate the semantic perplexity on such as NER tags.\n",
    "    n_gram: int = 1, # The n-gram to calculate the perplexity on.\n",
    "    batch_size: int = 1, # The batch size to use when calculating the perplexity.\n",
    "    num_proc: int = os.cpu_count(), # The number of processes to use when tokenizing the dataset.\n",
    "    device: str = \"cuda\", # The device to use when calculating the perplexity.\n",
    "    collate_fn = default_data_collator, # The collate function to use when calculating the perplexity.\n",
    "    return_tokens: bool = False, # Whether to return the tokens counts along with the perplexity.\n",
    "    return_distributions: bool = False, # Whether to return the perplexity distributions instead of the perplexity.\n",
    "): # The perplexity of the model on the dataset or a tuple of the perplexity and the token counts.\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a model on a dataset.\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = model.config.tokenizer_class.from_pretrained(model.config.pretrained_model_name_or_path)\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    batched = batch_size > 1\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenizer(x[column], truncation=True, padding=\"max_length\"),\n",
    "        batched=batched,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=dataset.column_names,\n",
    "        num_proc=num_proc,\n",
    "    )\n",
    "\n",
    "    # Create a dataloader for the dataset\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Calculate the perplexity of the model on the dataset\n",
    "    total_loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    total_token_cnt = Counter()\n",
    "    for batch in track(dataloader, description=\"Calculating perplexity\"):\n",
    "        # Move the batch to the device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        loss_cnt, token_cnt = get_counts(model, tokenizer, batch, semantic_column, return_distributions)\n",
    "        for token, loss in loss_cnt.items():\n",
    "            total_loss_cnt[token] += loss\n",
    "        total_token_cnt += token_cnt\n",
    "    \n",
    "    # Calculate the perplexity\n",
    "    perplexity = defaultdict(list) if return_distributions else Counter()\n",
    "    for token, loss in total_loss_cnt.items():\n",
    "        if return_distributions:\n",
    "            perplexity[token] = list(map(lambda x: 2 ** x, loss))\n",
    "        else:\n",
    "            perplexity[token] = torch.exp(torch.tensor(loss / total_token_cnt[token])).item()\n",
    "    \n",
    "    if return_tokens:\n",
    "        return perplexity, total_token_cnt\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9769e73f0000d75f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c06d16b4af418b8c5b4733f3c985be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3565ac6ee3c44fbab416eda27e214ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model.to(\"cpu\")\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\").select(range(50))\n",
    "# filter out empty strings\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "perplexity_cnt, token_cnt = perplexed(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    column=\"text\",\n",
    "    batch_size=2,\n",
    "    device=\"cpu\",\n",
    "    num_proc=1,\n",
    "    return_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' wired', 60983220.0),\n",
       " (' shatter', 12281874.0),\n",
       " (' Career', 4064274.5),\n",
       " (' Early', 2422943.75),\n",
       " (' Television', 2325893.0),\n",
       " (' Daylight', 2126348.5),\n",
       " (' unrecogn', 1731038.5),\n",
       " (' @', 1636278.125),\n",
       " (' Chou', 1440191.125),\n",
       " (' advisers', 1118558.375)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', 52832),\n",
       " (' the', 114),\n",
       " (',', 107),\n",
       " ('.', 83),\n",
       " (' \"', 72),\n",
       " (' in', 69),\n",
       " (' of', 52),\n",
       " (' a', 44),\n",
       " (' =', 41),\n",
       " (' and', 40)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|endoftext|>': 30566.95703125\n",
      "' the': 4.492285251617432\n",
      "',': 16.437183380126953\n",
      "'.': 9.6357421875\n",
      "' \"': 9.516890525817871\n",
      "' in': 7.487175941467285\n",
      "' of': 3.44857120513916\n",
      "' a': 8.229166984558105\n",
      "' =': 51.09266662597656\n",
      "' and': 5.262118339538574\n"
     ]
    }
   ],
   "source": [
    "# perplexity of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {perplexity_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity per semantic type\n",
    "\n",
    "The following cells contain the code for calculating the perplexity per semantic type of a tokenizer for aligning the AST of a program with the BPE of a language model's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration codeparrot--codeparrot-clean-valid-826c6fd8b27e5523\n",
      "Found cached dataset json (/home/nathan/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-valid-826c6fd8b27e5523/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "Parameter 'function'=<function perplexed.<locals>.<lambda>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ebdc230c1c47d29d39664c77553d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4308bbaef1c426e884151f19b100ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from code_tokenizers.core import CodeTokenizer\n",
    "\n",
    "def code_collator(batch):\n",
    "    merged_ast = []\n",
    "    for b in batch:\n",
    "        merged_ast.append(b.pop(\"merged_ast\"))\n",
    "    batch = default_data_collator(batch)\n",
    "    batch[\"merged_ast\"] = merged_ast\n",
    "    return batch\n",
    "\n",
    "model_name = \"codeparrot/codeparrot-small\"\n",
    "py_tokenizer = CodeTokenizer.from_pretrained(model_name, \"python\")\n",
    "py_tokenizer.node_types.append(\"as_pattern_target\")\n",
    "py_tokenizer.tokenizer.pad_token = py_tokenizer.tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"codeparrot/codeparrot-clean-valid\", split=\"train\").select(range(10))\n",
    "perplexity_cnt, token_cnt = perplexed(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer=py_tokenizer,\n",
    "    column=\"content\",\n",
    "    semantic_column=\"merged_ast\",\n",
    "    batch_size=1,\n",
    "    num_proc=1,\n",
    "    device=\"cpu\",\n",
    "    collate_fn=code_collator,\n",
    "    return_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reports', 4496803.0),\n",
       " ('Double', 4140204.0),\n",
       " ('BLANK', 3750757.25),\n",
       " ('BD', 1003989.75),\n",
       " ('CO', 805091.3125),\n",
       " ('Pure', 525452.5625),\n",
       " ('customize', 488133.78125),\n",
       " (' inte', 449579.28125),\n",
       " (' ways', 434788.71875),\n",
       " (' filenames', 432984.59375)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<module -> comment>', 3023),\n",
       " ('<|endoftext|>', 2557),\n",
       " ('<import_from_statement -> from>', 819),\n",
       " ('< N/A >', 667),\n",
       " ('<argument_list -> string>', 575),\n",
       " ('<attribute -> identifier>', 551),\n",
       " ('<expression_statement -> string>', 489),\n",
       " ('<dotted_name -> identifier>', 463),\n",
       " ('_', 361),\n",
       " ('.', 355)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<module -> comment>': 403.2505187988281\n",
      "'<|endoftext|>': 9550.5166015625\n",
      "'<import_from_statement -> from>': 5880.80029296875\n",
      "'< N/A >': 4.412940502166748\n",
      "'<argument_list -> string>': 8.250838279724121\n",
      "'<attribute -> identifier>': 1.6259663105010986\n",
      "'<expression_statement -> string>': 9.665909767150879\n",
      "'<dotted_name -> identifier>': 2.7604565620422363\n",
      "'_': 1.3683202266693115\n",
      "'.': 1.3909730911254883\n"
     ]
    }
   ],
   "source": [
    "# perplexity of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {perplexity_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('perplexed')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
