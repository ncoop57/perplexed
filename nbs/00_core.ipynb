{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> This module contains the core functions for calculating the perplexity of a language model per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import datasets\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from rich.progress import track\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from datasets import load_dataset\n",
    "from nbdev.showdoc import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def loss_func(\n",
    "    logits,  # the model's output\n",
    "    labels,  # the labels to calculate the cross entropy loss against\n",
    "):  # the loss per token of shape (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    Calculates the cross entropy loss for the model's output and the labels.\n",
    "    \"\"\"\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    loss = loss.view(*shift_labels.size())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3432, 3.7964, 6.6038, 1.7265, 5.4809],\n",
       "        [2.3432, 3.7964, 6.6038, 1.7265, 5.4809]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test loss function\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "inputs = tokenizer(\n",
    "    [\"Hello, my dog is cute\", \"Hello, my dog is cute\"], return_tensors=\"pt\"\n",
    ")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "labels = inputs.input_ids\n",
    "loss_func(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_counts(\n",
    "    model,  # the model to use for predictions\n",
    "    tokenizer,  # the tokenizer to use for encoding\n",
    "    batch,  # the batch to use for predictions\n",
    "    semantic_column: str,  # the column to use for semantic predictions\n",
    "    stop_word_column: str,  # the column to use for stop word predictions\n",
    "    return_distributions: bool,  # whether to return the distributions\n",
    "):  # the counts for the losses and tokens\n",
    "    \"\"\"\n",
    "    Returns the counts for the losses and tokens.\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids, attention_mask=attention_mask, labels=input_ids, return_dict=True\n",
    "        )\n",
    "    loss = loss_func(outputs.logits, input_ids)\n",
    "\n",
    "    # Add the losses to the counter for each\n",
    "    # token in the input\n",
    "    loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    token_cnt = Counter()\n",
    "    for i, ids in enumerate(input_ids):\n",
    "        for j, token in enumerate(ids[1:]):\n",
    "            # Skip the stop words\n",
    "            if stop_word_column != None:\n",
    "                stop_word = batch[stop_word_column][i][j]\n",
    "                if stop_word:\n",
    "                    continue\n",
    "\n",
    "            token = tokenizer.decode(token)\n",
    "            loss_cnt[token] += (\n",
    "                [loss[i][j].item()] if return_distributions else loss[i][j].item()\n",
    "            )\n",
    "            token_cnt[token] += 1\n",
    "\n",
    "            if semantic_column != None and token != tokenizer.pad_token:\n",
    "                semantic = batch[semantic_column][i][j]\n",
    "                loss_cnt[semantic] += (\n",
    "                    [loss[i][j].item()] if return_distributions else loss[i][j].item()\n",
    "                )\n",
    "                token_cnt[semantic] += 1\n",
    "\n",
    "    return loss_cnt, token_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def perplexed(\n",
    "    model: transformers.PreTrainedModel,  # The model to calculate the perplexity of.\n",
    "    dataset: datasets.Dataset,  # The dataset to calculate the perplexity on.\n",
    "    tokenizer: transformers.PreTrainedTokenizer = None,  # The tokenizer to use to tokenize the dataset. If not provided, the tokenizer associated with the model will be used.\n",
    "    column: str = \"text\",  # The column of the dataset to calculate the perplexity on.\n",
    "    semantic_column: str = None,  # The column of the dataset to calculate the semantic perplexity on such as NER tags.\n",
    "    stop_word_column: str = None,  # The column of the dataset that contains boolean values indicating whether the token is a stop word.\n",
    "    n_gram: int = 1,  # The n-gram to calculate the perplexity on.\n",
    "    batch_size: int = 1,  # The batch size to use when calculating the perplexity.\n",
    "    num_proc: int = os.cpu_count(),  # The number of processes to use when tokenizing the dataset.\n",
    "    device: str = \"cuda\",  # The device to use when calculating the perplexity.\n",
    "    collate_fn=default_data_collator,  # The collate function to use when calculating the perplexity.\n",
    "    pass_row: bool = False,  # Whether to pass the row to the tokenizer.\n",
    "    return_tokens: bool = False,  # Whether to return the tokens counts along with the perplexity.\n",
    "    return_distributions: bool = False,  # Whether to return the perplexity distributions instead of the perplexity.\n",
    "    compute_perplexity: bool = True,  # Whether to compute the perplexity. If False, the cross entropy will be returned instead.\n",
    "):  # The perplexity of the model on the dataset or a tuple of the perplexity and the token counts.\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a model on a dataset.\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = model.config.tokenizer_class.from_pretrained(\n",
    "            model.config.pretrained_model_name_or_path\n",
    "        )\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    batched = batch_size > 1\n",
    "    tokenize = (\n",
    "        lambda x: tokenizer(x[column], truncation=True, padding=\"max_length\")\n",
    "        if not pass_row\n",
    "        else tokenizer(x, truncation=True, padding=\"max_length\")\n",
    "    )\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenize(x),\n",
    "        batched=batched,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=dataset.column_names,\n",
    "        num_proc=num_proc,\n",
    "        desc=\"Tokenizing dataset\",\n",
    "    )\n",
    "\n",
    "    # Create a dataloader for the dataset\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Calculate the perplexity of the model on the dataset\n",
    "    total_loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    total_token_cnt = Counter()\n",
    "    for batch in track(dataloader, description=\"Calculating perplexity\"):\n",
    "        # Move the batch to the device\n",
    "        batch = {\n",
    "            k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "            for k, v in batch.items()\n",
    "        }\n",
    "        loss_cnt, token_cnt = get_counts(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            batch,\n",
    "            semantic_column,\n",
    "            stop_word_column,\n",
    "            return_distributions,\n",
    "        )\n",
    "        for token, loss in loss_cnt.items():\n",
    "            total_loss_cnt[token] += loss\n",
    "        total_token_cnt += token_cnt\n",
    "\n",
    "    # Calculate the perplexity\n",
    "    perplexity = defaultdict(list) if return_distributions else Counter()\n",
    "    for token, loss in total_loss_cnt.items():\n",
    "        if compute_perplexity:\n",
    "            if return_distributions:\n",
    "                perplexity[token] = list(\n",
    "                    map(lambda x: torch.exp(torch.tensor(x)).item(), loss)\n",
    "                )\n",
    "            else:\n",
    "                perplexity[token] = torch.exp(\n",
    "                    torch.tensor(loss / total_token_cnt[token])\n",
    "                ).item()\n",
    "        else:\n",
    "            if return_distributions:\n",
    "                perplexity[token] = loss\n",
    "            else:\n",
    "                perplexity[token] = loss / total_token_cnt[token]\n",
    "\n",
    "    if return_tokens:\n",
    "        return perplexity, total_token_cnt\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874da8041b4d41189ea3a10b63533b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model.to(device)\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\").select(range(50))\n",
    "# filter out empty strings\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "perplexity_cnt, token_cnt = perplexed(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    column=\"text\",\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    num_proc=1,\n",
    "    return_tokens=True,\n",
    ")\n",
    "assert len(perplexity_cnt) == len(token_cnt)\n",
    "assert perplexity_cnt.keys() == token_cnt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98872cc7ff1a43d5bafd9a0fa1a9122a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_cnt, token_cnt = perplexed(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    column=\"text\",\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    num_proc=1,\n",
    "    return_tokens=True,\n",
    "    compute_perplexity=False,\n",
    ")\n",
    "assert len(cross_cnt) == len(token_cnt)\n",
    "assert cross_cnt.keys() == token_cnt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' wired', 17.92612648010254),\n",
       " (' shatter', 16.32363510131836),\n",
       " (' Career', 15.21772575378418),\n",
       " (' Early', 14.70047664642334),\n",
       " (' Television', 14.659582138061523),\n",
       " (' Daylight', 14.56997299194336),\n",
       " (' unrecogn', 14.364179611206055),\n",
       " (' @', 14.307954322208058),\n",
       " (' Chou', 14.180266380310059),\n",
       " (' advisers', 13.927596092224121)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mers', 0.03539723251014948),\n",
       " ('mith', 0.018193976022303104),\n",
       " ('t', 0.016906073316931725),\n",
       " (' than', 0.009314415045082569),\n",
       " ('jiang', 0.005416479427367449),\n",
       " ('ian', 0.004262291360646486),\n",
       " ('aire', 0.002999095479026437),\n",
       " ('el', 0.0017088347813114524),\n",
       " ('ights', 0.001490435330197215),\n",
       " ('sworth', 0.0009158230968751013)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_cnt.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|endoftext|>': 10.327683209001043\n",
      "' the': 1.5023754525995046\n",
      "',': 2.799564078589466\n",
      "'.': 2.2654987903962653\n",
      "' \"': 2.2530801612883806\n",
      "' in': 2.0132113315057065\n",
      "' of': 1.2379778898500193\n",
      "' a': 2.107695746828209\n",
      "' =': 3.9336307379530697\n",
      "' and': 1.6605487003922463\n"
     ]
    }
   ],
   "source": [
    "# cross entropy of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {cross_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity per semantic type\n",
    "\n",
    "The following cells contain the code for calculating the perplexity per semantic type of a tokenizer for aligning the AST of a program with the BPE of a language model's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U code_tokenizers\n",
    "!download_grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_tokenizers.core import CodeTokenizer\n",
    "\n",
    "def code_collator(batch):\n",
    "    merged_ast = []\n",
    "    for b in batch:\n",
    "        merged_ast.append(b.pop(\"merged_ast\"))\n",
    "    batch = default_data_collator(batch)\n",
    "    batch[\"merged_ast\"] = merged_ast\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c756f3d331340faaab28d0858b6726f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"codeparrot/codeparrot-small\"\n",
    "py_tokenizer = CodeTokenizer.from_pretrained(model_name, \"python\")\n",
    "py_tokenizer.tokenizer.pad_token = py_tokenizer.tokenizer.eos_token\n",
    "py_tokenizer.pad_token = py_tokenizer.tokenizer.pad_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "dataset = load_dataset(\"codeparrot/codeparrot-clean-valid\", split=\"train\").select(\n",
    "    range(15)\n",
    ")\n",
    "cross_cnt, token_cnt = perplexed(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer=py_tokenizer,\n",
    "    column=\"content\",\n",
    "    semantic_column=\"merged_ast\",\n",
    "    stop_word_column=\"is_builtins\",\n",
    "    batch_size=1,\n",
    "    num_proc=1,\n",
    "    device=device,\n",
    "    collate_fn=code_collator,\n",
    "    return_tokens=True,\n",
    "    compute_perplexity=False,\n",
    ")\n",
    "\n",
    "assert len(cross_cnt) == len(token_cnt)\n",
    "assert cross_cnt.keys() == token_cnt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reports', 15.318881034851074),\n",
       " ('Double', 15.236268043518066),\n",
       " ('BLANK', 15.137480735778809),\n",
       " ('148', 14.469829559326172),\n",
       " ('BD', 13.819499969482422),\n",
       " ('year', 13.65689468383789),\n",
       " (' filesystem', 13.625283241271973),\n",
       " ('CO', 13.59871768951416),\n",
       " ('Pure', 13.172009468078613),\n",
       " ('customize', 13.098344802856445)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', 3951),\n",
       " ('<module -> comment>', 1479),\n",
       " ('< N/A >', 1123),\n",
       " ('<attribute -> identifier>', 1019),\n",
       " ('<argument_list -> string>', 728),\n",
       " ('<expression_statement -> string>', 677),\n",
       " ('.', 608),\n",
       " ('<dotted_name -> identifier>', 608),\n",
       " ('_', 434),\n",
       " ('\\n', 391)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|endoftext|>': 30567.21875\n",
      "'<module -> comment>': 0\n",
      "'< N/A >': 0\n",
      "'<attribute -> identifier>': 0\n",
      "'<argument_list -> string>': 0\n",
      "'<expression_statement -> string>': 0\n",
      "'.': 9.635930061340332\n",
      "'<dotted_name -> identifier>': 0\n",
      "'_': 0\n",
      "'\n",
      "': 3.0456223487854004\n"
     ]
    }
   ],
   "source": [
    "# perplexity of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {perplexity_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perplexed",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
