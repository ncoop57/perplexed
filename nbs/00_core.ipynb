{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from rich.progress import track\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datasets import load_dataset\n",
    "from nbdev.showdoc import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def loss_func(logits, labels):\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # reshape to (batch_size, sequence_length)\n",
    "    loss = loss.view(*shift_labels.size())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_counts(model, tokenizer, batch, semantic_column: str, return_distributions: bool):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    # input_ids = torch.tensor(batch[\"input_ids\"])\n",
    "    # attention_mask = torch.tensor(batch[\"attention_mask\"])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids, return_dict=True)\n",
    "    loss = loss_func(outputs.logits, input_ids)\n",
    "    # print(loss.shape)\n",
    "    # print(input_ids.shape)\n",
    "    # Add the losses to the counter for each \n",
    "    # token in the input\n",
    "    loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    token_cnt = Counter()\n",
    "    for i, ids in enumerate(input_ids):\n",
    "        for j, token in enumerate(ids[1:]):\n",
    "            token = tokenizer.decode(token)\n",
    "            loss_cnt[token] += [loss[i][j].item()] if return_distributions else loss[i][j].item()\n",
    "            token_cnt[token] += 1\n",
    "\n",
    "            if semantic_column != None:\n",
    "                semantic = batch[semantic_column][i]\n",
    "                loss_cnt[semantic] += [loss[i].item()] if return_distributions else loss[i].item()\n",
    "                token_cnt[semantic] += 1\n",
    "    # for i, token in enumerate(input_ids[1:]):\n",
    "    #     token = tokenizer.decode(token)\n",
    "    #     loss_cnt[token] += [loss[i].item()] if return_distributions else loss[i].item()\n",
    "    #     token_cnt[token] += 1\n",
    "    \n",
    "    #     if semantic_column != None:\n",
    "    #         semantic = batch[semantic_column][i]\n",
    "    #         loss_cnt[semantic] += [loss[i].item()] if return_distributions else loss[i].item()\n",
    "    #         token_cnt[semantic] += 1\n",
    "    return loss_cnt, token_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def perplexed(\n",
    "    model: transformers.PreTrainedModel, # The model to calculate the perplexity of.\n",
    "    dataset: datasets.Dataset, # The dataset to calculate the perplexity on.\n",
    "    tokenizer: transformers.PreTrainedTokenizer = None, # The tokenizer to use to tokenize the dataset. If not provided, the tokenizer associated with the model will be used.\n",
    "    column: str = \"text\", # The column of the dataset to calculate the perplexity on.\n",
    "    semantic_column: str = None, # The column of the dataset to calculate the semantic perplexity on such as NER tags.\n",
    "    n_gram: int = 1, # The n-gram to calculate the perplexity on.\n",
    "    batch_size: int = 1, # The batch size to use when calculating the perplexity.\n",
    "    device: str = \"cuda\", # The device to use when calculating the perplexity.\n",
    "    collate_fn = default_data_collator, # The collate function to use when calculating the perplexity.\n",
    "    return_tokens: bool = False, # Whether to return the tokens counts along with the perplexity.\n",
    "    return_distributions: bool = False, # Whether to return the perplexity distributions instead of the perplexity.\n",
    "): # The perplexity of the model on the dataset or a tuple of the perplexity and the token counts.\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a model on a dataset.\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = model.config.tokenizer_class.from_pretrained(model.config.pretrained_model_name_or_path)\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    batched = batch_size > 1\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenizer(x[column], truncation=True, padding=\"max_length\"),\n",
    "        batched=batched,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    # Create a dataloader for the dataset\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Calculate the perplexity of the model on the dataset\n",
    "    total_loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    total_token_cnt = Counter()\n",
    "    for batch in track(dataloader, description=\"Calculating perplexity\"):\n",
    "        # Move the batch to the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        loss_cnt, token_cnt = get_counts(model, tokenizer, batch, semantic_column, return_distributions)\n",
    "        for token, loss in loss_cnt.items():\n",
    "            total_loss_cnt[token] += loss\n",
    "        total_token_cnt += token_cnt\n",
    "    \n",
    "    # Calculate the perplexity\n",
    "    perplexity = defaultdict(list) if return_distributions else Counter()\n",
    "    for token, loss in total_loss_cnt.items():\n",
    "        if return_distributions:\n",
    "            perplexity[token] = list(map(lambda x: 2 ** x, loss))\n",
    "        else:\n",
    "            perplexity[token] = torch.exp(torch.tensor(loss / total_token_cnt[token])).item()\n",
    "    \n",
    "    if return_tokens:\n",
    "        return perplexity, total_token_cnt\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-539070ede7844002.arrow\n",
      "Loading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-62d1e97981c3439b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32e0c32aefd4a47b8f102a4abe5832f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model.to(\"cpu\")\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\").select(range(10))\n",
    "# filter out empty strings\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "perplexity_cnt, token_cnt = perplexed(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    column=\"text\",\n",
    "    batch_size=2,\n",
    "    device=\"cpu\",\n",
    "    return_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' @', 58884616.0),\n",
       " (' Career', 4064274.5),\n",
       " (' Citizenship', 2621984.0),\n",
       " (' Kier', 998199.375),\n",
       " (' Daylight', 385076.9375),\n",
       " (' Craig', 367194.25),\n",
       " (' Teddy', 301091.5),\n",
       " (' Bush', 247001.234375),\n",
       " (' Donkey', 122835.953125),\n",
       " (' Firm', 120278.03125)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', 9843),\n",
       " (' the', 19),\n",
       " (' in', 18),\n",
       " (',', 15),\n",
       " ('.', 14),\n",
       " (' by', 10),\n",
       " (' =', 9),\n",
       " (' a', 9),\n",
       " (' television', 8),\n",
       " (' B', 7)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from code_tokenizers.core import CodeTokenizer\n",
    "\n",
    "# model_name = \"codeparrot/codeparrot-small\"\n",
    "# py_tokenizer = CodeTokenizer.from_pretrained(model_name, \"python\")\n",
    "# py_tokenizer.node_types.append(\"as_pattern_target\")\n",
    "# py_tokenizer.tokenizer.pad_token = py_tokenizer.tokenizer.eos_token\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# dataset = load_dataset(\"codeparrot/codeparrot-clean-valid\", split=\"train\").select(range(10))\n",
    "# perplexity_cnt, token_cnt = perplexed(\n",
    "#     model,\n",
    "#     dataset,\n",
    "#     tokenizer=py_tokenizer,\n",
    "#     column=\"content\",\n",
    "#     semantic_column=\"merged_ast\",\n",
    "#     batch_size=1,\n",
    "#     device=\"cpu\",\n",
    "#     return_tokens=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {perplexity_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {perplexity_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('perplexed')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
