{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> This module contains the core functions for calculating the perplexity of a language model per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from rich.progress import track\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datasets import load_dataset\n",
    "from nbdev.showdoc import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy_with_logits(logits, labels):\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    logits = np.clip(logits, -50, 50)  # Clamp the logits to the range [-100, 100]\n",
    "    labels = np.eye(logits.shape[1])[labels]\n",
    "    # Transform the logits into predictions using the softmax function\n",
    "    predictions = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    epsilon = 1e-7\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    # Calculate the cross entropy between the predictions and the labels\n",
    "    cross_entropy = -(labels * np.log(predictions + epsilon))\n",
    "    # return torch.tensor(cross_entropy)\n",
    "    # print(cross_entropy)\n",
    "    # grab the non zero values\n",
    "    # print(cross_entropy)\n",
    "    cross_entropy = cross_entropy[np.nonzero(cross_entropy)]\n",
    "    # remove nan values\n",
    "    cross_entropy = cross_entropy[~np.isnan(cross_entropy)]\n",
    "    return torch.tensor(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4076, 0.6931], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([[1., 2., 3.], [4., 50., 700.]])\n",
    "labels = torch.tensor([2, 1])\n",
    "cross_entropy_with_logits(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1206074/3386334810.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = CrossEntropyLoss(reduction=\"mean\")(torch.tensor(logits), torch.tensor(labels))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(325.2038)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = CrossEntropyLoss(reduction=\"mean\")(torch.tensor(logits), torch.tensor(labels))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1206074/2969991533.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  F.cross_entropy(torch.tensor(logits), torch.tensor(labels))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(325.2038)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(torch.tensor(logits), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1206074/4285881817.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = CrossEntropyLoss(reduction=\"none\")(torch.tensor(logits), torch.tensor(labels))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4.0761e-01, 6.5000e+02])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross entropy experiments\n",
    "loss = CrossEntropyLoss(reduction=\"none\")(torch.tensor(logits), torch.tensor(labels))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def loss_func(\n",
    "    logits,                 # the model's output\n",
    "    labels,                 # the labels to calculate the cross entropy loss against\n",
    "    use_custom_loss=False   # whether to use the custom loss function\n",
    "):                          # the loss per token of shape (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    Calculates the cross entropy loss for the model's output and the labels.\n",
    "    \"\"\"\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    if use_custom_loss:\n",
    "        loss = cross_entropy_with_logits(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    else:\n",
    "        loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    loss = loss.view(*shift_labels.size())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3432, 3.7964, 6.6038, 1.7265, 5.4809],\n",
       "        [2.3432, 3.7964, 6.6038, 1.7265, 5.4809]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test loss function\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "inputs = tokenizer([\"Hello, my dog is cute\", \"Hello, my dog is cute\"], return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "labels = inputs.input_ids\n",
    "loss_func(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_counts(\n",
    "    model,                      # the model to use for predictions\n",
    "    tokenizer,                  # the tokenizer to use for encoding\n",
    "    batch,                      # the batch to use for predictions\n",
    "    semantic_column: str,       # the column to use for semantic predictions\n",
    "    return_distributions: bool  # whether to return the distributions\n",
    "):                              # the counts for the losses and tokens\n",
    "    \"\"\"\n",
    "    Returns the counts for the losses and tokens.\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids, return_dict=True)\n",
    "    loss = loss_func(outputs.logits, input_ids)\n",
    "\n",
    "    # Add the losses to the counter for each \n",
    "    # token in the input\n",
    "    loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    token_cnt = Counter()\n",
    "    for i, ids in enumerate(input_ids):\n",
    "        for j, token in enumerate(ids[1:]):\n",
    "            token = tokenizer.decode(token)\n",
    "            loss_cnt[token] += [loss[i][j].item()] if return_distributions else loss[i][j].item()\n",
    "            token_cnt[token] += 1\n",
    "\n",
    "            if semantic_column != None:\n",
    "                semantic = batch[semantic_column][i][j]\n",
    "                loss_cnt[semantic] += [\n",
    "                    loss[i][j].item()\n",
    "                ] if return_distributions else loss[i][j].item()\n",
    "                token_cnt[semantic] += 1\n",
    "\n",
    "    return loss_cnt, token_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def perplexed(\n",
    "    model: transformers.PreTrainedModel, # The model to calculate the perplexity of.\n",
    "    dataset: datasets.Dataset, # The dataset to calculate the perplexity on.\n",
    "    tokenizer: transformers.PreTrainedTokenizer = None, # The tokenizer to use to tokenize the dataset. If not provided, the tokenizer associated with the model will be used.\n",
    "    column: str = \"text\", # The column of the dataset to calculate the perplexity on.\n",
    "    semantic_column: str = None, # The column of the dataset to calculate the semantic perplexity on such as NER tags.\n",
    "    n_gram: int = 1, # The n-gram to calculate the perplexity on.\n",
    "    batch_size: int = 1, # The batch size to use when calculating the perplexity.\n",
    "    num_proc: int = os.cpu_count(), # The number of processes to use when tokenizing the dataset.\n",
    "    device: str = \"cuda\", # The device to use when calculating the perplexity.\n",
    "    collate_fn = default_data_collator, # The collate function to use when calculating the perplexity.\n",
    "    pass_row: bool = False, # Whether to pass the row to the tokenizer.\n",
    "    return_tokens: bool = False, # Whether to return the tokens counts along with the perplexity.\n",
    "    return_distributions: bool = False, # Whether to return the perplexity distributions instead of the perplexity.\n",
    "    compute_perplexity: bool = True, # Whether to compute the perplexity. If False, the cross entropy will be returned instead.\n",
    "): # The perplexity of the model on the dataset or a tuple of the perplexity and the token counts.\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a model on a dataset.\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = model.config.tokenizer_class.from_pretrained(model.config.pretrained_model_name_or_path)\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    batched = batch_size > 1\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenizer(x[column], truncation=True, padding=\"max_length\")\n",
    "        if not pass_row else tokenizer(x, truncation=True, padding=\"max_length\"),\n",
    "        batched=batched,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=dataset.column_names,\n",
    "        num_proc=num_proc,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "\n",
    "    # Create a dataloader for the dataset\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Calculate the perplexity of the model on the dataset\n",
    "    total_loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    total_token_cnt = Counter()\n",
    "    for batch in track(dataloader, description=\"Calculating perplexity\"):\n",
    "        # Move the batch to the device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        loss_cnt, token_cnt = get_counts(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            batch,\n",
    "            semantic_column,\n",
    "            return_distributions\n",
    "        )\n",
    "        for token, loss in loss_cnt.items():\n",
    "            total_loss_cnt[token] += loss\n",
    "        total_token_cnt += token_cnt\n",
    "    \n",
    "    # Calculate the perplexity\n",
    "    perplexity = defaultdict(list) if return_distributions else Counter()\n",
    "    for token, loss in total_loss_cnt.items():\n",
    "        if compute_perplexity:\n",
    "            if return_distributions:\n",
    "                perplexity[token] = list(map(lambda x: torch.exp(torch.tensor(x)).item(), loss))\n",
    "            else:\n",
    "                perplexity[token] = torch.exp(torch.tensor(loss / total_token_cnt[token])).item()\n",
    "        else:\n",
    "            if return_distributions:\n",
    "                perplexity[token] = loss\n",
    "            else:\n",
    "                perplexity[token] = loss / total_token_cnt[token]\n",
    "    \n",
    "    if return_tokens:\n",
    "        return perplexity, total_token_cnt\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9769e73f0000d75f.arrow\n",
      "Loading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1a1b0afe3c4d00a0.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112bd38d365b4f38adc2bd9d8167a4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model.to(\"cpu\")\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\").select(range(50))\n",
    "# filter out empty strings\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "perplexity_cnt, token_cnt = perplexed(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    column=\"text\",\n",
    "    batch_size=1,\n",
    "    device=\"cpu\",\n",
    "    num_proc=1,\n",
    "    return_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' wired', 60983220.0),\n",
       " (' shatter', 12281874.0),\n",
       " (' Career', 4064274.5),\n",
       " (' Early', 2422943.75),\n",
       " (' Television', 2325893.0),\n",
       " (' Daylight', 2126348.5),\n",
       " (' unrecogn', 1731038.5),\n",
       " (' @', 1636278.125),\n",
       " (' Chou', 1440191.125),\n",
       " (' advisers', 1118558.375)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mers', 1.0360186100006104),\n",
       " ('mith', 1.0183566808700562),\n",
       " ('t', 1.0170230865478516),\n",
       " (' than', 1.009339451789856),\n",
       " ('jiang', 1.0054292678833008),\n",
       " ('ian', 1.0042657852172852),\n",
       " ('aire', 1.0030004978179932),\n",
       " ('el', 1.001706838607788),\n",
       " ('ights', 1.0014889240264893),\n",
       " ('sworth', 1.0009148120880127)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_cnt.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', 52832),\n",
       " (' the', 114),\n",
       " (',', 107),\n",
       " ('.', 83),\n",
       " (' \"', 72),\n",
       " (' in', 69),\n",
       " (' of', 52),\n",
       " (' a', 44),\n",
       " (' =', 41),\n",
       " (' and', 40)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|endoftext|>': 30566.95703125\n",
      "' the': 4.492285251617432\n",
      "',': 16.437183380126953\n",
      "'.': 9.6357421875\n",
      "' \"': 9.516890525817871\n",
      "' in': 7.487175941467285\n",
      "' of': 3.44857120513916\n",
      "' a': 8.229166984558105\n",
      "' =': 51.09266662597656\n",
      "' and': 5.262118339538574\n"
     ]
    }
   ],
   "source": [
    "# perplexity of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {perplexity_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity per semantic type\n",
    "\n",
    "The following cells contain the code for calculating the perplexity per semantic type of a tokenizer for aligning the AST of a program with the BPE of a language model's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: code_tokenizers in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (0.0.4)\n",
      "Requirement already satisfied: fastcore in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code_tokenizers) (1.5.27)\n",
      "Requirement already satisfied: gitpython in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code_tokenizers) (3.1.29)\n",
      "Requirement already satisfied: transformers in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code_tokenizers) (4.24.0)\n",
      "Requirement already satisfied: tree-sitter==0.20.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code_tokenizers) (0.20.1)\n",
      "Requirement already satisfied: pandas in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code_tokenizers) (1.5.1)\n",
      "Requirement already satisfied: pip in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from fastcore->code_tokenizers) (22.2.2)\n",
      "Requirement already satisfied: packaging in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from fastcore->code_tokenizers) (21.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from gitpython->code_tokenizers) (4.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from pandas->code_tokenizers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from pandas->code_tokenizers) (2022.6)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from pandas->code_tokenizers) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (0.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (0.13.2)\n",
      "Requirement already satisfied: requests in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (3.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (6.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->code_tokenizers) (5.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers->code_tokenizers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from packaging->fastcore->code_tokenizers) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->code_tokenizers) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code_tokenizers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code_tokenizers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code_tokenizers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code_tokenizers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install code_tokenizers\n",
    "!download_grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration codeparrot--codeparrot-clean-valid-826c6fd8b27e5523\n",
      "Found cached dataset json (/home/nathan/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-valid-826c6fd8b27e5523/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6b6e3bdcdb4086bddd061b3e74b2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b8a67558704ced8117b413fccd4dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from code_tokenizers.core import CodeTokenizer\n",
    "\n",
    "def code_collator(batch):\n",
    "    merged_ast = []\n",
    "    for b in batch:\n",
    "        merged_ast.append(b.pop(\"merged_ast\"))\n",
    "    batch = default_data_collator(batch)\n",
    "    batch[\"merged_ast\"] = merged_ast\n",
    "    return batch\n",
    "\n",
    "model_name = \"codeparrot/codeparrot-small\"\n",
    "py_tokenizer = CodeTokenizer.from_pretrained(model_name, \"python\")\n",
    "py_tokenizer.node_types.append(\"as_pattern_target\")\n",
    "py_tokenizer.tokenizer.pad_token = py_tokenizer.tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"codeparrot/codeparrot-clean-valid\", split=\"train\").select(range(10))\n",
    "perplexity_cnt, token_cnt = perplexed(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer=py_tokenizer,\n",
    "    column=\"content\",\n",
    "    semantic_column=\"merged_ast\",\n",
    "    batch_size=1,\n",
    "    num_proc=1,\n",
    "    device=\"cpu\",\n",
    "    collate_fn=code_collator,\n",
    "    return_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reports', 3101943.25),\n",
       " ('Double', 2928004.0),\n",
       " ('BLANK', 2727696.0),\n",
       " ('BD', 912393.3125),\n",
       " ('CO', 745110.5625),\n",
       " ('Pure', 499221.3125),\n",
       " ('customize', 465415.96875),\n",
       " (' inte', 430242.40625),\n",
       " (' ways', 416678.46875),\n",
       " (' filenames', 415019.5625)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<module -> comment>', 3023),\n",
       " ('<|endoftext|>', 2557),\n",
       " ('<import_from_statement -> from>', 819),\n",
       " ('< N/A >', 667),\n",
       " ('<argument_list -> string>', 575),\n",
       " ('<attribute -> identifier>', 551),\n",
       " ('<expression_statement -> string>', 489),\n",
       " ('<dotted_name -> identifier>', 463),\n",
       " ('_', 361),\n",
       " ('.', 355)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<module -> comment>': 402.9506530761719\n",
      "'<|endoftext|>': 9540.9033203125\n",
      "'<import_from_statement -> from>': 5876.1806640625\n",
      "'< N/A >': 4.411830425262451\n",
      "'<argument_list -> string>': 8.249506950378418\n",
      "'<attribute -> identifier>': 1.6259615421295166\n",
      "'<expression_statement -> string>': 9.663031578063965\n",
      "'<dotted_name -> identifier>': 2.7417614459991455\n",
      "'_': 1.3683314323425293\n",
      "'.': 1.3909822702407837\n"
     ]
    }
   ],
   "source": [
    "# perplexity of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {perplexity_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('perplexed')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "166acbafcfc170fc9e76f9068a80e60ac012edbf58e088a9a9f871dfc99226f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
