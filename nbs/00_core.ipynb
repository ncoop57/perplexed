{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datasets import load_dataset\n",
    "from nbdev.showdoc import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def loss_func(logits, labels):\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_counts(model, tokenizer, batch, semantic_column: str, return_distributions: bool):\n",
    "    input_ids = torch.tensor(batch[\"input_ids\"])\n",
    "    attention_mask = torch.tensor(batch[\"attention_mask\"])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids, return_dict=True)\n",
    "    loss = loss_func(outputs.logits, input_ids)\n",
    "    # Add the losses to the counter for each \n",
    "    # token in the input\n",
    "    loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    token_cnt = Counter()\n",
    "    for i, token in enumerate(input_ids[1:]):\n",
    "        token = tokenizer.decode(token)\n",
    "        loss_cnt[token] += [loss[i].item()] if return_distributions else loss[i].item()\n",
    "        token_cnt[token] += 1\n",
    "    \n",
    "        if semantic_column != None:\n",
    "            semantic = batch[semantic_column][i]\n",
    "            loss_cnt[semantic] += [loss[i].item()] if return_distributions else loss[i].item()\n",
    "            token_cnt[semantic] += 1\n",
    "    return loss_cnt, token_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def perplexed(\n",
    "    model: transformers.PreTrainedModel, # The model to calculate the perplexity of.\n",
    "    dataset: datasets.Dataset, # The dataset to calculate the perplexity on.\n",
    "    tokenizer: transformers.PreTrainedTokenizer = None, # The tokenizer to use to tokenize the dataset. If not provided, the tokenizer associated with the model will be used.\n",
    "    column: str = \"text\", # The column of the dataset to calculate the perplexity on.\n",
    "    semantic_column: str = None, # The column of the dataset to calculate the semantic perplexity on such as NER tags.\n",
    "    n_gram: int = 1, # The n-gram to calculate the perplexity on.\n",
    "    batch_size: int = 1, # The batch size to use when calculating the perplexity.\n",
    "    device: str = \"cuda\", # The device to use when calculating the perplexity.\n",
    "    return_tokens: bool = False, # Whether to return the tokens counts along with the perplexity.\n",
    "    return_distributions: bool = False, # Whether to return the perplexity distributions instead of the perplexity.\n",
    "): # The perplexity of the model on the dataset or a tuple of the perplexity and the token counts.\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a model on a dataset.\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = model.config.tokenizer_class.from_pretrained(model.config.pretrained_model_name_or_path)\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    batched = batch_size > 1\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenizer(x[column], truncation=True),\n",
    "        batched=batched,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    # TODO: Add support for semantic perplexity\n",
    "\n",
    "    # Calculate the perplexity of the model on the dataset\n",
    "    total_loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    total_token_cnt = Counter()\n",
    "    for batch in tokenized_dataset:\n",
    "        loss_cnt, token_cnt = get_counts(model, tokenizer, batch, semantic_column, return_distributions)\n",
    "        for token, loss in loss_cnt.items():\n",
    "            total_loss_cnt[token] += loss\n",
    "        total_token_cnt += token_cnt\n",
    "    \n",
    "    # Calculate the perplexity\n",
    "    perplexity = defaultdict(list) if return_distributions else Counter()\n",
    "    for token, loss in total_loss_cnt.items():\n",
    "        if return_distributions:\n",
    "            perplexity[token] = list(map(lambda x: 2 ** x, loss))\n",
    "        else:\n",
    "            perplexity[token] = torch.exp(torch.tensor(loss / total_token_cnt[token])).item()\n",
    "    \n",
    "    if return_tokens:\n",
    "        return perplexity, total_token_cnt\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration codeparrot--codeparrot-clean-valid-826c6fd8b27e5523\n",
      "Found cached dataset json (/home/nathan/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-valid-826c6fd8b27e5523/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "Parameter 'function'=<function perplexed.<locals>.<lambda>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 5/5 [00:27<00:00,  5.48s/ex]\n"
     ]
    }
   ],
   "source": [
    "from code_tokenizers.core import CodeTokenizer\n",
    "\n",
    "model_name = \"codeparrot/codeparrot-small\"\n",
    "py_tokenizer = CodeTokenizer.from_pretrained(model_name, \"python\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"codeparrot/codeparrot-clean-valid\", split=\"train\").select(range(5))\n",
    "perplexity_cnt, token_cnt = perplexed(\n",
    "    model, dataset,\n",
    "    tokenizer=py_tokenizer,\n",
    "    column=\"content\",\n",
    "    semantic_column=\"merged_ast\",\n",
    "    batch_size=1,\n",
    "    device=\"cpu\",\n",
    "    return_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reports', 4496803.0),\n",
       " ('Double', 4140204.0),\n",
       " ('Pure', 525452.5625),\n",
       " (' inte', 449579.28125),\n",
       " (' ways', 434788.71875),\n",
       " (' filenames', 432984.59375),\n",
       " ('segments', 418304.90625),\n",
       " ('FN', 415667.96875),\n",
       " (' Con', 381470.75),\n",
       " ('conflict', 329534.9375)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<module -> comment>', 945),\n",
       " ('<expression_statement -> string>', 413),\n",
       " ('< N/A >', 313),\n",
       " ('<attribute -> identifier>', 243),\n",
       " ('.', 196),\n",
       " ('<argument_list -> string>', 178),\n",
       " ('<dotted_name -> identifier>', 166),\n",
       " ('\\n', 155),\n",
       " ('_', 149),\n",
       " (',', 147)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<module -> comment>': 4.602316856384277\n",
      "'<expression_statement -> string>': 13.976248741149902\n",
      "'< N/A >': 4.582609176635742\n",
      "'<attribute -> identifier>': 1.4728107452392578\n",
      "'.': 1.4726667404174805\n",
      "'<argument_list -> string>': 3.3904495239257812\n",
      "'<dotted_name -> identifier>': 4.801618576049805\n",
      "'\n",
      "': 1.9747979640960693\n",
      "'_': 1.451112985610962\n",
      "',': 1.654050350189209\n"
     ]
    }
   ],
   "source": [
    "# perplexity of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {perplexity_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-fc80113c96285cf5.arrow\n",
      "100%|██████████| 61/61 [00:00<00:00, 4654.40ex/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\").select(range(100))\n",
    "# filter out empty strings\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "perplexity_cnt, token_cnt = perplexed(model, dataset, tokenizer=tokenizer, column=\"text\", batch_size=1, device=\"cpu\", return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' wired', 60983452.0),\n",
       " (' 768', 21569920.0),\n",
       " (' shatter', 12281851.0),\n",
       " (' unsett', 8289411.5),\n",
       " (' ignited', 6605284.5),\n",
       " (' Tanz', 4834806.5),\n",
       " (' Influence', 4153385.0),\n",
       " (' Career', 4064134.75),\n",
       " (' Television', 2325895.0),\n",
       " (' Moral', 2243555.25)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 240),\n",
       " (' the', 231),\n",
       " ('.', 169),\n",
       " (' of', 142),\n",
       " (' \"', 124),\n",
       " (' in', 123),\n",
       " (' and', 93),\n",
       " (' to', 91),\n",
       " (' his', 87),\n",
       " (' a', 81)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "',': 21.052648544311523\n",
      "' the': 4.730246543884277\n",
      "'.': 10.683691024780273\n",
      "' of': 2.6912641525268555\n",
      "' \"': 17.342397689819336\n",
      "' in': 9.113482475280762\n",
      "' and': 6.846656322479248\n",
      "' to': 2.7567787170410156\n",
      "' his': 11.905125617980957\n",
      "' a': 8.68340015411377\n"
     ]
    }
   ],
   "source": [
    "# perplexity of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {perplexity_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('perplexed')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
