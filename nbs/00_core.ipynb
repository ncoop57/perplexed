{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> This module contains the core functions for calculating the perplexity of a language model per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from rich.progress import track\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datasets import load_dataset\n",
    "from nbdev.showdoc import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def loss_func(\n",
    "    logits,                 # the model's output\n",
    "    labels,                 # the labels to calculate the cross entropy loss against\n",
    "    use_custom_loss=False   # whether to use the custom loss function\n",
    "):                          # the loss per token of shape (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    Calculates the cross entropy loss for the model's output and the labels.\n",
    "    \"\"\"\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1)\n",
    "    )\n",
    "    loss = loss.view(*shift_labels.size())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3432, 3.7964, 6.6038, 1.7265, 5.4809],\n",
       "        [2.3432, 3.7964, 6.6038, 1.7265, 5.4809]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test loss function\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "inputs = tokenizer([\"Hello, my dog is cute\", \"Hello, my dog is cute\"], return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "labels = inputs.input_ids\n",
    "loss_func(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_counts(\n",
    "    model,                      # the model to use for predictions\n",
    "    tokenizer,                  # the tokenizer to use for encoding\n",
    "    batch,                      # the batch to use for predictions\n",
    "    semantic_column: str,       # the column to use for semantic predictions\n",
    "    return_distributions: bool  # whether to return the distributions\n",
    "):                              # the counts for the losses and tokens\n",
    "    \"\"\"\n",
    "    Returns the counts for the losses and tokens.\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "    loss = loss_func(outputs.logits, input_ids)\n",
    "\n",
    "    # Add the losses to the counter for each \n",
    "    # token in the input\n",
    "    loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    token_cnt = Counter()\n",
    "    for i, ids in enumerate(input_ids):\n",
    "        for j, token in enumerate(ids[1:]):\n",
    "            token = tokenizer.decode(token)\n",
    "            loss_cnt[token] += [loss[i][j].item()] if return_distributions else loss[i][j].item()\n",
    "            token_cnt[token] += 1\n",
    "\n",
    "            if semantic_column != None and token != tokenizer.pad_token:\n",
    "                semantic = batch[semantic_column][i][j]\n",
    "                loss_cnt[semantic] += [\n",
    "                    loss[i][j].item()\n",
    "                ] if return_distributions else loss[i][j].item()\n",
    "                token_cnt[semantic] += 1\n",
    "\n",
    "    return loss_cnt, token_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def perplexed(\n",
    "    model: transformers.PreTrainedModel, # The model to calculate the perplexity of.\n",
    "    dataset: datasets.Dataset, # The dataset to calculate the perplexity on.\n",
    "    tokenizer: transformers.PreTrainedTokenizer = None, # The tokenizer to use to tokenize the dataset. If not provided, the tokenizer associated with the model will be used.\n",
    "    column: str = \"text\", # The column of the dataset to calculate the perplexity on.\n",
    "    semantic_column: str = None, # The column of the dataset to calculate the semantic perplexity on such as NER tags.\n",
    "    n_gram: int = 1, # The n-gram to calculate the perplexity on.\n",
    "    batch_size: int = 1, # The batch size to use when calculating the perplexity.\n",
    "    num_proc: int = os.cpu_count(), # The number of processes to use when tokenizing the dataset.\n",
    "    device: str = \"cuda\", # The device to use when calculating the perplexity.\n",
    "    collate_fn = default_data_collator, # The collate function to use when calculating the perplexity.\n",
    "    pass_row: bool = False, # Whether to pass the row to the tokenizer.\n",
    "    return_tokens: bool = False, # Whether to return the tokens counts along with the perplexity.\n",
    "    return_distributions: bool = False, # Whether to return the perplexity distributions instead of the perplexity.\n",
    "    compute_perplexity: bool = True, # Whether to compute the perplexity. If False, the cross entropy will be returned instead.\n",
    "): # The perplexity of the model on the dataset or a tuple of the perplexity and the token counts.\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a model on a dataset.\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = model.config.tokenizer_class.from_pretrained(model.config.pretrained_model_name_or_path)\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    batched = batch_size > 1\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenizer(x[column], truncation=True, padding=\"max_length\")\n",
    "        if not pass_row else tokenizer(x, truncation=True, padding=\"max_length\"),\n",
    "        batched=batched,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=dataset.column_names,\n",
    "        num_proc=num_proc,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "\n",
    "    # Create a dataloader for the dataset\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Calculate the perplexity of the model on the dataset\n",
    "    total_loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    total_token_cnt = Counter()\n",
    "    for batch in track(dataloader, description=\"Calculating perplexity\"):\n",
    "        # Move the batch to the device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        loss_cnt, token_cnt = get_counts(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            batch,\n",
    "            semantic_column,\n",
    "            return_distributions\n",
    "        )\n",
    "        for token, loss in loss_cnt.items():\n",
    "            total_loss_cnt[token] += loss\n",
    "        total_token_cnt += token_cnt\n",
    "    \n",
    "    # Calculate the perplexity\n",
    "    perplexity = defaultdict(list) if return_distributions else Counter()\n",
    "    for token, loss in total_loss_cnt.items():\n",
    "        if compute_perplexity:\n",
    "            if return_distributions:\n",
    "                perplexity[token] = list(map(lambda x: torch.exp(torch.tensor(x)).item(), loss))\n",
    "            else:\n",
    "                perplexity[token] = torch.exp(torch.tensor(loss / total_token_cnt[token])).item()\n",
    "        else:\n",
    "            if return_distributions:\n",
    "                perplexity[token] = loss\n",
    "            else:\n",
    "                perplexity[token] = loss / total_token_cnt[token]\n",
    "    \n",
    "    if return_tokens:\n",
    "        return perplexity, total_token_cnt\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/transformers_cache/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at /transformers_cache/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-9769e73f0000d75f.arrow\n",
      "Loading cached processed dataset at /transformers_cache/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-0b3bb22b8be942e8.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dce7d56fcfe4d608faa3c82600719df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model.to(\"cpu\")\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\").select(range(50))\n",
    "# filter out empty strings\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "perplexity_cnt, token_cnt = perplexed(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    column=\"text\",\n",
    "    batch_size=1,\n",
    "    device=\"cpu\",\n",
    "    num_proc=1,\n",
    "    return_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' wired', 60982756.0),\n",
       " (' shatter', 12281804.0),\n",
       " (' Career', 4064324.75),\n",
       " (' Early', 2422916.0),\n",
       " (' Television', 2325886.25),\n",
       " (' Daylight', 2126374.75),\n",
       " (' unrecogn', 1731017.0),\n",
       " (' @', 1636278.125),\n",
       " (' Chou', 1440177.375),\n",
       " (' advisers', 1118574.375)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mers', 1.0360190868377686),\n",
       " ('mith', 1.0183565616607666),\n",
       " ('t', 1.017022728919983),\n",
       " (' than', 1.0093393325805664),\n",
       " ('jiang', 1.0054292678833008),\n",
       " ('ian', 1.0042656660079956),\n",
       " ('aire', 1.0030004978179932),\n",
       " ('el', 1.0017069578170776),\n",
       " ('ights', 1.0014889240264893),\n",
       " ('sworth', 1.0009146928787231)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_cnt.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', 52832),\n",
       " (' the', 114),\n",
       " (',', 107),\n",
       " ('.', 83),\n",
       " (' \"', 72),\n",
       " (' in', 69),\n",
       " (' of', 52),\n",
       " (' a', 44),\n",
       " (' =', 41),\n",
       " (' and', 40)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|endoftext|>': 30566.986328125\n",
      "' the': 4.492286205291748\n",
      "',': 16.437183380126953\n",
      "'.': 9.6357421875\n",
      "' \"': 9.51689338684082\n",
      "' in': 7.4871673583984375\n",
      "' of': 3.4485690593719482\n",
      "' a': 8.229151725769043\n",
      "' =': 51.0925178527832\n",
      "' and': 5.262114524841309\n"
     ]
    }
   ],
   "source": [
    "# perplexity of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {perplexity_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /transformers_cache/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-0b3bb22b8be942e8.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca20d4016db5489f99647485d29483cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cross_cnt, token_cnt \u001b[39m=\u001b[39m perplexed(\n\u001b[1;32m      2\u001b[0m     model,\n\u001b[1;32m      3\u001b[0m     dataset,\n\u001b[1;32m      4\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m      5\u001b[0m     column\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     num_proc\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     return_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     10\u001b[0m     compute_perplexity\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     11\u001b[0m )\n",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m, in \u001b[0;36mperplexed\u001b[0;34m(model, dataset, tokenizer, column, semantic_column, n_gram, batch_size, num_proc, device, collate_fn, pass_row, return_tokens, return_distributions, compute_perplexity)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m track(dataloader, description\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCalculating perplexity\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[39m# Move the batch to the device\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m---> 45\u001b[0m     loss_cnt, token_cnt \u001b[39m=\u001b[39m get_counts(\n\u001b[1;32m     46\u001b[0m         model,\n\u001b[1;32m     47\u001b[0m         tokenizer,\n\u001b[1;32m     48\u001b[0m         batch,\n\u001b[1;32m     49\u001b[0m         semantic_column,\n\u001b[1;32m     50\u001b[0m         return_distributions\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m token, loss \u001b[39min\u001b[39;00m loss_cnt\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     53\u001b[0m         total_loss_cnt[token] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mget_counts\u001b[0;34m(model, tokenizer, batch, semantic_column, return_distributions)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     16\u001b[0m     outputs \u001b[39m=\u001b[39m model(\n\u001b[1;32m     17\u001b[0m         input_ids,\n\u001b[1;32m     18\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m     19\u001b[0m         labels\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m     20\u001b[0m         return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 22\u001b[0m loss \u001b[39m=\u001b[39m loss_func(outputs\u001b[39m.\u001b[39;49mlogits, input_ids)\n\u001b[1;32m     24\u001b[0m \u001b[39m# Add the losses to the counter for each \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# token in the input\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss_cnt \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m) \u001b[39mif\u001b[39;00m return_distributions \u001b[39melse\u001b[39;00m Counter()\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mloss_func\u001b[0;34m(logits, labels, use_custom_loss)\u001b[0m\n\u001b[1;32m     11\u001b[0m shift_labels \u001b[39m=\u001b[39m labels[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m     12\u001b[0m loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss(reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[39m=\u001b[39m loss_fct(\n\u001b[1;32m     14\u001b[0m     shift_logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, shift_logits\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)),\n\u001b[1;32m     15\u001b[0m     shift_labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mview(\u001b[39m*\u001b[39mshift_labels\u001b[39m.\u001b[39msize())\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.conda/envs/perplexed/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/perplexed/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/.conda/envs/perplexed/lib/python3.10/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross_cnt, token_cnt = perplexed(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    column=\"text\",\n",
    "    batch_size=1,\n",
    "    device=\"cpu\",\n",
    "    num_proc=1,\n",
    "    return_tokens=True,\n",
    "    compute_perplexity=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' wired', 17.926109313964844),\n",
       " (' shatter', 16.32363510131836),\n",
       " (' Career', 15.217745780944824),\n",
       " (' Early', 14.700493812561035),\n",
       " (' Television', 14.659614562988281),\n",
       " (' Daylight', 14.569916725158691),\n",
       " (' unrecogn', 14.364232063293457),\n",
       " (' @', 14.307934696024114),\n",
       " (' Chou', 14.180286407470703),\n",
       " (' advisers', 13.92755126953125)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mers', 0.035385108552873135),\n",
       " ('mith', 0.018190178088843822),\n",
       " ('t', 0.016879817470908165),\n",
       " (' than', 0.009296108968555927),\n",
       " ('jiang', 0.005414582323282957),\n",
       " ('ian', 0.0042567127384245396),\n",
       " ('aire', 0.0029960053507238626),\n",
       " ('el', 0.001705383649095893),\n",
       " ('ights', 0.00148781668394804),\n",
       " ('sworth', 0.000914393924176693)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_cnt.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|endoftext|>': 10.327674743941088\n",
      "' the': 1.5023615921667792\n",
      "',': 2.799546029940944\n",
      "'.': 2.2654792655663316\n",
      "' \"': 2.253068277819289\n",
      "' in': 2.0131916974981627\n",
      "' of': 1.2379600099744634\n",
      "' a': 2.1076847396113654\n",
      "' =': 3.933640957242105\n",
      "' and': 1.6605336494743823\n"
     ]
    }
   ],
   "source": [
    "# cross entropy of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {cross_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity per semantic type\n",
    "\n",
    "The following cells contain the code for calculating the perplexity per semantic type of a tokenizer for aligning the AST of a program with the BPE of a language model's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: code_tokenizers in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (0.0.4)\n",
      "Requirement already satisfied: fastcore in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code_tokenizers) (1.5.27)\n",
      "Requirement already satisfied: gitpython in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code_tokenizers) (3.1.29)\n",
      "Requirement already satisfied: transformers in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code_tokenizers) (4.24.0)\n",
      "Requirement already satisfied: tree-sitter==0.20.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code_tokenizers) (0.20.1)\n",
      "Requirement already satisfied: pandas in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from code_tokenizers) (1.5.1)\n",
      "Requirement already satisfied: pip in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from fastcore->code_tokenizers) (22.2.2)\n",
      "Requirement already satisfied: packaging in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from fastcore->code_tokenizers) (21.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from gitpython->code_tokenizers) (4.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from pandas->code_tokenizers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from pandas->code_tokenizers) (2022.6)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from pandas->code_tokenizers) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (0.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (0.13.2)\n",
      "Requirement already satisfied: requests in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (3.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from transformers->code_tokenizers) (6.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->code_tokenizers) (5.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers->code_tokenizers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from packaging->fastcore->code_tokenizers) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->code_tokenizers) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code_tokenizers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code_tokenizers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code_tokenizers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages (from requests->transformers->code_tokenizers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install code_tokenizers\n",
    "!download_grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration codeparrot--codeparrot-clean-valid-826c6fd8b27e5523\n",
      "Found cached dataset json (/home/nathan/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-valid-826c6fd8b27e5523/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6b6e3bdcdb4086bddd061b3e74b2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b8a67558704ced8117b413fccd4dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from code_tokenizers.core import CodeTokenizer\n",
    "\n",
    "def code_collator(batch):\n",
    "    merged_ast = []\n",
    "    for b in batch:\n",
    "        merged_ast.append(b.pop(\"merged_ast\"))\n",
    "    batch = default_data_collator(batch)\n",
    "    batch[\"merged_ast\"] = merged_ast\n",
    "    return batch\n",
    "\n",
    "model_name = \"codeparrot/codeparrot-small\"\n",
    "py_tokenizer = CodeTokenizer.from_pretrained(model_name, \"python\")\n",
    "py_tokenizer.node_types.append(\"as_pattern_target\")\n",
    "py_tokenizer.tokenizer.pad_token = py_tokenizer.tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"codeparrot/codeparrot-clean-valid\", split=\"train\").select(range(10))\n",
    "perplexity_cnt, token_cnt = perplexed(\n",
    "    model,\n",
    "    dataset,\n",
    "    tokenizer=py_tokenizer,\n",
    "    column=\"content\",\n",
    "    semantic_column=\"merged_ast\",\n",
    "    batch_size=1,\n",
    "    num_proc=1,\n",
    "    device=\"cpu\",\n",
    "    collate_fn=code_collator,\n",
    "    return_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reports', 3101943.25),\n",
       " ('Double', 2928004.0),\n",
       " ('BLANK', 2727696.0),\n",
       " ('BD', 912393.3125),\n",
       " ('CO', 745110.5625),\n",
       " ('Pure', 499221.3125),\n",
       " ('customize', 465415.96875),\n",
       " (' inte', 430242.40625),\n",
       " (' ways', 416678.46875),\n",
       " (' filenames', 415019.5625)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<module -> comment>', 3023),\n",
       " ('<|endoftext|>', 2557),\n",
       " ('<import_from_statement -> from>', 819),\n",
       " ('< N/A >', 667),\n",
       " ('<argument_list -> string>', 575),\n",
       " ('<attribute -> identifier>', 551),\n",
       " ('<expression_statement -> string>', 489),\n",
       " ('<dotted_name -> identifier>', 463),\n",
       " ('_', 361),\n",
       " ('.', 355)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<module -> comment>': 402.9506530761719\n",
      "'<|endoftext|>': 9540.9033203125\n",
      "'<import_from_statement -> from>': 5876.1806640625\n",
      "'< N/A >': 4.411830425262451\n",
      "'<argument_list -> string>': 8.249506950378418\n",
      "'<attribute -> identifier>': 1.6259615421295166\n",
      "'<expression_statement -> string>': 9.663031578063965\n",
      "'<dotted_name -> identifier>': 2.7417614459991455\n",
      "'_': 1.3683314323425293\n",
      "'.': 1.3909822702407837\n"
     ]
    }
   ],
   "source": [
    "# perplexity of the most common tokens\n",
    "tokens = [token for token, _ in token_cnt.most_common(10)]\n",
    "for token in tokens:\n",
    "    print(f\"'{token}': {perplexity_cnt[token]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perplexed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7a35294fad52d4d7cb15d57ee5bbf2c260f7c0645bad8bee2fbb4acd72015c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
