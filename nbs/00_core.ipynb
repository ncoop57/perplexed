{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/perplexed/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datasets import load_dataset\n",
    "from nbdev.showdoc import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def loss_func(logits, labels):\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_counts(model, tokenizer, batch, return_distributions: bool = False):\n",
    "    input_ids = torch.tensor(batch[\"input_ids\"])\n",
    "    attention_mask = torch.tensor(batch[\"attention_mask\"])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids, return_dict=True)\n",
    "    loss = loss_func(outputs.logits, input_ids)\n",
    "    # Add the losses to the counter for each \n",
    "    # token in the input\n",
    "    loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    token_cnt = Counter()\n",
    "    for i, token in enumerate(input_ids[1:]):\n",
    "        token = tokenizer.decode(token)\n",
    "        loss_cnt[token] += [loss[i].item()] if return_distributions else loss[i].item()\n",
    "        token_cnt[token] += 1\n",
    "    \n",
    "        if semantic_column != None:\n",
    "            semantic = batch[semantic_column][i]\n",
    "            loss_cnt[semantic] += loss[i].item()\n",
    "            token_cnt[semantic] += 1\n",
    "    return loss_cnt, token_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def perplexed(\n",
    "    model: transformers.PreTrainedModel, # The model to calculate the perplexity of.\n",
    "    dataset: datasets.Dataset, # The dataset to calculate the perplexity on.\n",
    "    tokenizer: transformers.PreTrainedTokenizer = None, # The tokenizer to use to tokenize the dataset. If not provided, the tokenizer associated with the model will be used.\n",
    "    column: str = \"text\", # The column of the dataset to calculate the perplexity on.\n",
    "    semantic_column: str = None, # The column of the dataset to calculate the semantic perplexity on such as NER tags.\n",
    "    n_gram: int = 1, # The n-gram to calculate the perplexity on.\n",
    "    batch_size: int = 1, # The batch size to use when calculating the perplexity.\n",
    "    device: str = \"cuda\", # The device to use when calculating the perplexity.\n",
    "    return_tokens: bool = False, # Whether to return the tokens counts along with the perplexity.\n",
    "    return_distributions: bool = False, # Whether to return the perplexity distributions instead of the perplexity.\n",
    "): # The perplexity of the model on the dataset or a tuple of the perplexity and the token counts.\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a model on a dataset.\n",
    "    \"\"\"\n",
    "    if tokenizer is None:\n",
    "        tokenizer = model.config.tokenizer_class.from_pretrained(model.config.pretrained_model_name_or_path)\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenizer(x[column], return_tensors=\"pt\", truncation=True),\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    # TODO: Add support for semantic perplexity\n",
    "\n",
    "    # Calculate the perplexity of the model on the dataset\n",
    "    total_loss_cnt = defaultdict(list) if return_distributions else Counter()\n",
    "    total_token_cnt = Counter()\n",
    "    for batch in tokenized_dataset:\n",
    "        loss_cnt, token_cnt = get_counts(model, tokenizer, batch, return_distributions)\n",
    "        for token, loss in loss_cnt.items():\n",
    "            total_loss_cnt[token] += loss\n",
    "        total_token_cnt += token_cnt\n",
    "    \n",
    "    # Calculate the perplexity\n",
    "    perplexity = defaultdict(list) if return_distributions else Counter()\n",
    "    for token, loss in total_loss_cnt.items():\n",
    "        if return_distributions:\n",
    "            perplexity[token] = list(map(lambda x: 2 ** x, loss))\n",
    "        else:\n",
    "            perplexity[token] = torch.exp(torch.tensor(loss / total_token_cnt[token])).item()\n",
    "    \n",
    "    if return_tokens:\n",
    "        return perplexity, total_token_cnt\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at /home/nathan/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-90706e68dc2e97c1.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\").select(range(1_000))\n",
    "# filter out empty strings\n",
    "dataset = dataset.filter(lambda x: len(x[\"text\"]) > 0)\n",
    "\n",
    "perplexity_cnt, token_cnt = perplexed(model, dataset, tokenizer=tokenizer, column=\"text\", batch_size=1, device=\"cpu\", return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "perplexity_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' the', 3535),\n",
       " (',', 2735),\n",
       " ('.', 2408),\n",
       " (' of', 1766),\n",
       " (' and', 1457),\n",
       " (' in', 1118),\n",
       " (' to', 1093),\n",
       " (' a', 831),\n",
       " (' @', 795),\n",
       " ('@', 795)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('perplexed')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
